# Problem Description
მოცემულ პროექტში ჩვენი მიზანია, ვიწინასწარმეტყველოთ Walmart-ის სხვადასხვა ფილიალის კვირეული გაყიდვები დეპარტამენტების მიხედვით. მოცემული მონაცემები წარმოადგენს დროის მწკრივებს და ისტორიული ინფორმაცია გვაქვს წარსულში განსხვავებული ფილიალის დეპარტამენტებში გაყიდვებთან დაკავშირებით. ჩვენ ვცადეთ ტრადიციული ARIMA-ს გამოყენება მოცემული ამოცანის გადასაჭრელად.


მონაცემების მომზადების ნაწილში გამოვიყვანეთ 4 სხვადასხვა დღესასწაულის ინდიკატორული ცვლადი, რადგან აღნიშნულ დღესასწაულების დღეებში გაყიდვების პროგნოზირება მნიშვნელოვანია. ასევე მონაცემებში აღმოჩენილი იყო სამიზნე ცვლადის უარყოფითი მნიშვნელობები, რომლებიც გადავყარეთ. რაც შეეხება გამოტოვებულ მნიშვნელობებს, მოდელირებისთვის საჭირო ცვლადებში ის არ შეგხვდა(ის მხოლოდ Markdown ტიპის ცვლადებშია, რომლებიც inference-ის დროს არ გვაქვს. ტრადიციული ARIMA-ს შემთხვევაში, განსხვავებით tft-სგან, ამის გამოყენება არ შეგვიძლია).


# Data Exploration
მონაცემების მოკვლევის ეტაპზე გაკეთდა:
1. სეზონური დეკომპოზიცია(seasonal decomposition), რათა ვიზუალურად წარმოგვედგინა მთელი სამიზნე ცვლადის განაწილება, ტრენდის ნაწილი, სეზონურობის ნაწილი და ხმაური

   <img width="655" height="500" alt="image" src="https://github.com/user-attachments/assets/f686f18a-f5ef-46e8-9ea0-1acf35236242" />

მოცემულ გრაფიკზე ჩვენ ვხედავთ პიკებს, რომლებიც სეზონური შოპინგის პერიოდებითაა გამოწვეული. ტრენდი თავიდან მეტნაკლებად სტაბილურია, შემდგომ მცირდება და ბოლოს იზრდება. შესაძლოა აქ გაყიდვების ქცევა შეიცვალა, რაც ეკონომიკურ ფაქტორებთანაც იყოს კავშირში.
ჩვენ ასევე ვხედავთ ყოველკვირეულ/ყოველწლიურ სეზონურობასაც. ხმაურის ნაწილში კი არაა მკვეთრად გამოკვეთილი კანონზომიერება, თუმცა ვხედავთ რამდენიმე ანომალურ მნიშვნელობასაც. 

მოცემული დროითი მწკრივის მონაცემები აშკარად არაა სტაციონარული.

2. ჩვენ სამიზნე ცვლადის lag-ის ცვლადებს გამოვიყვანეთ.
3. ვცადეთ გადიფერენცირება, რათა შეგვემოწმებინა დროის მწკრივის სტაციონარულობა.  ამისთვის ავიღეთ შემთხვევითი შერჩევა და 1-ელ გავადიფერენცირეთ მონაცემები, რის შემდეგაც დიკი ფულერის კრიტერიუმით დროის მწკრივი გახდა სტაციონარული(თავიდან არ იყო).

4. ავაგეთ ავტოკორელაციის და ნაწილობრივი ავტოკორელაციის გრაფიკები
   <img width="582" height="808" alt="image" src="https://github.com/user-attachments/assets/c3758e11-9058-4dcc-aef8-3024b30d28a9" />
გრაფიკიდან გამომდინარე, თითქმის ყველა ლაგი(გარდა მე-7სა ხვდება ნდობის არეში, ხოლო მე-7 ლაგში არსებული ამოვარდნა სუსტ სეზონურ დამოკიდებულებაზე შეიძლება მიუთითებდეს, რომელსაც შემდგომ SARIMA მოდელში გამოვიყენებთ).


5. გამოვიყვანეთ წლის, თვის, კვირისა და კვარტლის ცვლადები, ასევე დღესასწაულის ინდიკატორის ცვლადები
   
6.შევამოწმეთ გარე ფაქტორების კორელაცია გაყიდვებთან
<img width="927" height="698" alt="image" src="https://github.com/user-attachments/assets/3dae7f73-f114-4298-94c3-5527761b29e0" />
მოცემული გრაფიკი არ მიუთითებს ძლიერ კორელაციაზე გარეგან ფაქტორებსა და გაყიდვებს შორის(პირდაპირი კორელაცია ორ სიდიდეს შორის)

7. ავაგეთ განაწილების გრაფიკები, რომლის შემდგომაც გავუმკლავდით ანომალურ მნიშვნელობებს
   <img width="1365" height="564" alt="image" src="https://github.com/user-attachments/assets/f5b6868d-ecd8-4dd1-bdc0-e7961c2ce0cf" />
თითოეული მაღაზიისა და დეპარტამენტისთვის განისაზღვრა წილი ასეთი ანომალური მნიშვნელობებისა(რომლებიც სცდება აღნიშნულ საზღვრებს) და ეს propensity პარამეტრები გამოვიყენეთ დამატებით ცვლადად.

8. გადავყარეთ MarkDown ტიპის ცვლადები მათი დიდი რაოდენობის უცნობი მნიშვნელობების გამო
   
9. გადავყარეთ უარყოფითი გაყიდვების მქონე ჩანაწერები


# Time Series მოდელები

ამ მოდელებს ბევრი უპირატესობა აქვთ სხვებთან შედარებით time series პრობლემების გადასაჭრელად. 
კერძოდ, მოდელის თითოეულ პარამეტრს ცხადად განსაზღვრული დანიშნულება აქვს, რაც პროგნოზირებადს ხდის ტრენინგის პროცესს.
Confidence Interval-ები მოდელებს მოყვება.
ასევე ნათლად იცი თუ რა დაშვებებს აკეთებს მოდელი შენს მონაცემებზე.
time series vs deep learning:
GPU-ს არ საჭიროებს, სწრაფად გადის train ფაზას.
პატარა ზომის მონაცემებთანაც შეუძლია მუშაობა
ნაკლებად გადის overfit-ში.

ამას გარდა, სეზონურობისა და ტრენდების აღმოსაჩენად იდეალური ვარიანტია time series მოდელის გამოყენება.

## ARIMA

სულ თავიდან მოდელების რაოდენობის სიმცირის თვალსაზრისით გამოვიყვანეთ თითოეულ ფილიალში ჯამური გაყიდვები(დეპარტამენტებს არ ვითვალისწინებთ) და ისე ავაგეთ ARIMA მოდელი. შემდგომ მიდგომა გავხადეთ უფრო გრანულარული და თითოეული ფილიალისა და დეპარტამენტის ჭრილში ავაგეთ ARIMA მოდელი (1, 1, 1) პარამეტრებით. საბოლოო მიდგომაში დავლოგეთ თითოეული კომბინაციის მიხედვით აგრეგირებული მეტრიკები, როგორიცაა MAE, MAPE, RMSE. მივიღეთ საშუალო MAE - 2471.4190470652425, საშუალო MAPE - 1.7976931348623157e+308, საშუალო RMSE - 3011.8117169451707. შემდგომ დავაგენერირეთ საბმიშენი და მივიღეთ weighted mae - 5171.16096 kaggle-ზე.


## SARIMA

გავტესტეთ 5 ფაზად, I - (1,1,1)x(1,1,1,52) მარტივი სეზონური სარიმა გარე ცვლადების გარეშე. II - (1,1,0)x(1,1,0,52) ამოვაკელით moving average კომპონენტი, გავამახვილეთ ყურადღება auto regressive პარამეტრზე (საუკეთესო შედეგი). III - დავამატეთ გარე ცვლადები ტემპერატურისა და ბენზინის ფასის. შევამოწმეთ და დავრწმუნდით, რომ გარე ცვლადებს არ ჰქონია გავლენა. IV - დავამატეთ დასვენების დღეები ეგზოგენურ ცვლადებად. V - სხვადასვა გასტატიკებული პარამეტრების სელექცია. 
პირველადი შედეგი აღმოჩნდა საინტერესო, ყველა მოდელი, რომელიც MA-ს ითვალისწინებდა წავიდა საკმაოდ დიდ overfit-ში და მივიღეთ კოლოსალურად დიდი შედეგები, ისეთი, რომ გრაფიკზეც არ გამოჩნდა.
<img width="1164" height="104" alt="image" src="https://github.com/user-attachments/assets/0e1370c9-1ee1-461f-804e-8ae4aee95a77" />

<img width="1345" height="431" alt="image" src="https://github.com/user-attachments/assets/9c962859-3f05-4bca-832c-e955aa4ee78b" />

<img width="1447" height="477" alt="image" src="https://github.com/user-attachments/assets/0ff6d62a-fe47-437d-b92b-c9e3a952179d" />

ზედა სურათის მიხედვით AIC ყველა მოდელს კარგი აქვს, ანუ სატრენინგო მონაცემებს ყველა კარგად მოერგო.
ბოლო მეტრიკით კი შევადარეთ სხვა მოდელებთან რა შედეგი აქვს SARIMA-ს.

ამის შემდგომ, ცხადია, ლოგიკური იქნებოდა, რომ იგივე პარამეტრებზე გაგვეტესტა ოღონდ moving average-ის გარეშე, ასეც მოვიქეცით, თუმცა საუკეთესო შედეგი მაინც მეორე ფაზამ დადო.

თუმცა უნდა ვთქვათ, რომ რადგან SARIMA-ს competitive შედეგი არ ჰქონდა და რეალურად თანამედროვე მოდელებთან არც ექნებოდა, SARIMA დიდ განსხვავებას არ მოგვცემდა. შესაბამისად, დიდი რესურსის ჩადება SARIMA-ში არ გადავწყვიტეთ, რის გამოც ავიღეთ Store x Department-ების 50 კომბინაცია და მათზე გავაკეთეთ SARIMA-ს მოდელები. რის გამოც, inference-ს დროს prediction-ებში მოგვიწია median მნიშვნელობების შეტანა იმ Store-ებზე, რომლებზეც არ იყო დატრენინგებული SARIMA. შედეგად, როგორც მოსალოდნელი იყო, მივიღეთ საკმაოდ ცუდი შედეგი wmae - 16670.55259


## Prophet

ერთ-ერთი საუკეთესო ვარიანტი როდესაც საქმე ეხება ბიზნესს, მაგალითად, გაყიდვებს როგორც ჩვენს შემთხვევაში. მოხმარება საკმაოდ მარტივია და მინიმალურ სტატისტიკას მოითხოვს. თავისით შეუძლია missing data-სა და outliers-ების აღმოფხვრა (განსხვავებით მაგალითად ARIMA, SARIMA-სგან), შესაბამისად არ საჭიროებს data preprocessing-ს. პოულობს პატერნებს ავტომატურად. ცუდია იმ მხრივ, რომ აკეთებს წრფივ დაშვებას, შესაბამისად ექსპონენციალურ ან რთულ არაწრფივ ტრენდებთან მუშაობა გაუჭირდება. საჭიროებს კარგ historical data-ს, რათა პერფორმანსი დამაკმაყოფილებელი იყოს.
Prophet-მა Wallmart-ის მონაცემებზე კარგად იმუშავა, რადგან Wallmart-ის data გამოირჩევა სეზონურობით, რაც Prophet-ის შედეგებს უფრო ზუსტს ხდის. Prophet-ს არა Time Series მოდელებთან შედარებით უპირატესია იმ მხრივ, რომ დროითი დამოკიდებულების აღქმა შეუძლია, ამას დამატებული სეზონურობისა და ტრენდების აღმოჩენა, რასაც მაგალითად XGBoost-ში მხოლოდ დამატებითი feature-ების შემოყვანით მივაღწევდით.

ჩვენ გავტესტეთ Prophet-ის 4 პროგრესული ფაზა. I - ვითვალისწინებდით მხოლოდ ტრენდსა და სეზონურობას (საუკეთესო შედეგი) II - დავამატეთ დღესასწაულების მოდელირება, მიზანი იყო, რომ გვენახა, ექნებოდა თუ არა გავლენა დღესასწაულებს. მივიღეთ თითქმის იდენტური, ოდნავ უარესი შედეგი. III - გარე ცვლადები დავამატეთ, დღესასწაულები, ტემპერატურა, საწვავის ფასი, უმუშევრობის დონე. მიზანი იყო შეგვემოწმებინა თუ გარშემო ფაქტორებს (ეკონომიკურსა და ამინდის) რამდენად დიდი გავლენა ექნებოდა წინასწარმეტყველებაზე. IV - უკვე დავამატეთ ყველა feature,  და მივიღეთ თითქმის იდენტური შედეგი რაც III. მოდელი შევქმენით შემდეგი კონფიგურაციით: daily_seasonality=False, weekly_seasonality=True, yearly_seasonality=True, holidays=holidays_df, changepoint_prior_scale=0.05 (ანუ კონსერვატიული მიდგომა ავირჩიეთ ტრენდების ცვლილების მიმართ და seasonality_prior_scale=10.0

მივიღეთ შედეგები:
<img width="655" height="111" alt="image" src="https://github.com/user-attachments/assets/fc0354be-8bb1-43fe-bb84-fc99096315c5" />

საბოლოოდ დავაგენერირეთ kaggle-ს submission პირველი ფაზის მიხედვით და მივიღეთ wmae - 3262.03505

# Tree Based Models 

## LightGBM
tree based მოდელებთა სამუშაოდ გამოვიყენეთ იტერაციული მიდგომა, თავდაპირველად ვცადეთ ყველანაირი დამატებითი ფიჩერის გარეშე დაგვეტრეინინგებინა მოდელი,
გამოვიყენეთ მხოლოდ 3 ფიჩერი, Store, Dept, IsHoliday, რამაც გვაჩვენა რომ მოდელს უჭირს კონტექსტის აღქმა, არ აქვს წარსულის ან სეზონურობის გათვალისწინება.

მეორე ნაბიჯზე კი დავამატეთ შედარებით მარტივი დროითი ფიჩერები როგორებიცაა Year, Month, Week და ა.შ, ამით ვცადეთ დაგვენახა სეზონური და კალენდრული ეფექტების მნიშვნელობა, თუმცა
გაუმჯობესება არ დაფიქსირებულა, საინტერესოა ის ფაქტიც რომ ვალიდაციის სეტზე ზუსტად იგივე შდეგი მივიღეთ რაც წინა შემთხვევაში. ესეიგი შედეგის გასაუმჯობესებლად გვჭირდებოდა სხვა დამატებითი ფიჩერები 

<img width="177" height="23" alt="image" src="https://github.com/user-attachments/assets/5f8fa344-f1b3-4d1c-89ea-80f85d11ef90" />
<img width="183" height="20" alt="image" src="https://github.com/user-attachments/assets/11dc4880-acb4-4124-8dcb-524c8a613ecf" />

ამის შემდეგ გადავწყვიტეთ სხვა გზით გვეცადა მოდელისთვის დაგვენახებინა გაყიდვების short-term ან long-term პატერნი, რის გამოცა დავამატეთ lag ფიჩერები, ყოველკვირეულ გაყიდვებზე
lag feature ების დამატებამ ყოველკვირეულ გაყიდვებზე მოგვცა ძალიან კარგი შედეგი და MAE დაახლოებით 60 პროცენტით შემცირდა, აქ უკვე ნათელი იყო რომ მოდელს შეეძლო დამახსოვრებინა დროითი პატეერნები
<img width="203" height="31" alt="image" src="https://github.com/user-attachments/assets/312373c4-bb44-444b-8443-1e3e544a441b" />

<img width="525" height="388" alt="image" src="https://github.com/user-attachments/assets/c14d2f55-2a4a-41b7-96a6-d659a2755ed1" />
<img width="657" height="380" alt="image" src="https://github.com/user-attachments/assets/daad68ae-3992-4809-be69-1c5c3e7f2da9" />

ამის შემდეგ კი უბრალოდ ვცადეთ კიდევ რამდენიმიე ფიჩერის დამატება, მაგალითად რამდენიმე holidays აღმნიშვნელი ფიჩერის დამატება, რამდენიმე ახალი lag feature-ის დამატებაც, ('Fuel_Price_lag_1', 'Temperature_lag_1')
და ა.შ, ვალიდაციის სეტზე ოდნავ უკეთესი შედეგი შეიმჩნეოდა თუმცა არც ძალიან დიდად შემცირებული

ამის შემდეგ კი ვცადეთ ტესტ სეტზე შემოწმება და საკმაოდ ცუდი შედეგი მივიღეთ
<img width="723" height="73" alt="image" src="https://github.com/user-attachments/assets/2feba3aa-cb91-4890-b674-4d9128dc8e84" />

რა თქმა უნდა ამ დროს დავიწყეთ პრობლემის ძებნა, და ერთ-ერთი რაც აღმოვაჩინე იყო რომ ვალიდაციისა და ტრეინინგის სეტები არა დროით არამედ რენდომულად მქონდა გაყოფილი 80/20-ზე, ამ პრობლემის გამოსწორებამ მოდელს ტესტ სეტზე
შედეგს არუშველა, ამიტომ ჩავთვალე რომ პრობლემა lag ფიჩერებში იყო და ნელნ-ნელა დავიწყე მათი მოშორება, საბოლოოდ რამდენიმე ნაბიჯის შემდეგ მივიღეთ უფრო ცუდი MAE ვალიდაციის სეტზე, მაგრამ მივიღეთ უკეთესი შედეგი ტესტ სეტზე 

<img width="240" height="25" alt="image" src="https://github.com/user-attachments/assets/ddc603e4-1350-40f3-bff4-2405b47e0bc8" />
<img width="612" height="60" alt="image" src="https://github.com/user-attachments/assets/2b9fe9e6-41f0-42a9-9619-ffa8ca9e5ca5" />

აქ ძალიან გვიან მივხვდი რომ პრობლემა არა lag ფიჩერებში იყო, არამდე არასწორი იყო მიდგომა რომლის მიხედვითაც lag ფიჩერებს ვამატებდი test სეტზე, სადაც არანაირი იფნრომაცია არ გვქონია Weekl_sale ებზე, თუმცა ამ პრობლემის გამოსწორება შემდეგ მოდელში ვცადეთ.

## XGBoost 
LightGBM მოდელის შემდეგ შედარებით მარტივი იყო ამ მოდელთან მუშაობა და პროცესის წარმართვა, 

აქაც იტერაციულად, ნელ-ნელა მივედით საბოლოო მოდელამდე, რომელსაც საუკეთესო შედეგი ქონდა ვალიდაციის სეტზე,
გამოვიყენეთ იგივე ფიჩერები რათა მოდელს შეძლებოდა დროითი პატერნების დაჭერა. და საბოლოოდ მივედით ქვემოთ მოცემულ სიამდე. 

features = [
    'Store', 'Dept', 'IsHoliday', 'Month', 'Year', 'Week', 'Quarter',
    'Temperature', 'Fuel_Price', 'Size', 'Type',
    'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas',
    'Sales_Lag_1', 'Sales_Lag_2', 'Sales_Lag_3', 'Sales_Lag_4', 'Sales_Lag_52'
]

ამ ფიჩერებით დატრენინგებულმა მოდელმა ვალიდაციის სეტზე აჩვენა MAE: 1313 

<img width="288" height="34" alt="image" src="https://github.com/user-attachments/assets/de103d8f-0ab3-4267-bf51-ad692145e5d5" />

თუმცა ტესტ სეტზე, LightGBM ის მსგავსა აჩვენა ძალიან ცუდი შედეგი  

<img width="541" height="65" alt="image" src="https://github.com/user-attachments/assets/856badf8-1c43-47ef-86a4-0fffbb514775" />

რეალურად აქ დავფიქრდი, რამეს არასწორად ხომ არ ვაკეთებდი model_inference-ს ნაწილში, და შევცვალე მიდგომა, იმისა თუ როგორ ვამატებდი lag ველების მნიშვნელობას ტესტ სეტში, რამაც უკეთესობისკენ შეცვალა საერთო სურათი და ტესტ სეტზე მივიღეთ შემდეგი შედეგი. 
<img width="468" height="81" alt="image" src="https://github.com/user-attachments/assets/7773c95b-fe51-4802-a0e2-a389467b2a8d" />

დავმერჯეთ ტესტ სეტი და ტრეინინგის დროს გამოყენებული დატაფრეიმი, შემდეგ იტერაციულად გადავუყევით თითოეულ უნიკალურ თარიღს, თითეული თარიღისთვის ავიღეთ წინა კვირების (lag) გაყიდვები, შევავსეთ შესაბამისი ველები (fallback - ად ვავსებდით მედიანით), და მხოლოდ ამ კონკრეტულ ველზე 
ვცადეთ პრედიქშენის გაკეთება, მიღებული შედეგი უბრალოდ ჩავწერეთ დამერჯილ დატაფრეიმში, რადგან ეს მონაცემი აუცილებლად გამოგვადგებოდა სხვა თარიღების (lag) ფიჩერების შესამქნელად.

# Deep Learning vs Time Series Models

Deep learning-ს შეუძლია არაწრფივი კავშირის დამყარება წარსულსა და მომავალს შორის, განსხვავებით ARIMA, SARIMA და Prophet-ისგან, ასევე სეზონურობებს შორის კომპლექსური ურთიერთკავშირის აღმოჩენა შეუძლიათ. რაც მეტ დატას მიაწვდი, შედეგიც იზრდება, ტრადიციული time series მოდელები კი პიკს აღწევენ ამასობაში. აღმოჩენილი პატერნები A მაღაზიიდან გამოგვადგება B მაღაზიაში. თუმცა თუკი მონაცემები პატარაა, მაშინ Time Series მოდელებს აქვთ უპირატესობა. ასევე ქსელები ნელა ტრენინგდება განსხვავებით კლასიკური მოდელებისა.

## Temporal Fusion Transformer

ტრადიციულ Time Series მოდელებს აქვთ ფუნდამენტური შეზღუდვა, მათ ინფორმაციის ყველა ნაწილს თანაბრად აღიქვამენ. ხოლო TFT შეიქმნა ისე, რომ მოდელს აქვს საშუალება ყურადღება მიაქციოს ინფუთის გარკვეულ ნაწილებს. ეს არის შესაძლებელი ე.წ. Multi-Head Attention-ით, რომელიც attention მექანიზმით ანიჭებს შემოსული ინფორმაციის სხვადასხვა ნაწილებს სხვადასხვა წონებს. TFT-ში გვაქვს 3 პარამეტრი, Past Inputs, Future Known Inputs და Static Covariates. ინფორმაციის flow მიმდინარეობს LSTM-ის Encoder-იდან Decoder-მდე. LSTM Encoder input-ს იღებს Past Inputs-დან, ხოლო Decoder აგენერირებს ფრედიქშენს. LSTM-ის პასუხს ხვდება Add & Norm Gate, რომელიც ასტაბილურებს LSTM-ის output-ს, ამის შემდგომ ის გაივლის Gated Residual Network, რომელიც მუშაობს როგორც ფილტრი, ანუ მხოლოდ მნიშვნელოვან ინფორმაციას გაატარებს.

შევადაროთ მაგალითად N-BEAT-ს, რომელიც არის კლასიკური Time Series-ებისთვის, რომელიც არ საჭიროებს feature engineering-ს და არის ბევრად ჩქარი ვიდრე TFT, თუმცა Temporal Fusion Transformer-ში, ბევრი feature-ის ინტეგრაციაა შესაძლებელი, მათ შორის მათი წონების მინიჭებაც. ასევე შევადაროთ არა ნეირონულ ქსელს, მაგალითად, SARIMA-s=ს, რომელსაც სჭირდება სტაციონარული დატა თუმცა მუშაობს მცირე ზომის ინფორმაციაზეც. მაგრამ TFT-ს შეუძლია მოერგოს ნებისმიერ data distribution-ს, მაგრამ სჭირდება დიდი დატასეტი.

Temporal Fusion Transformer-ში მისი ტევადობის გათვალისწინებით გამოვიყენეთ მრავალი feature. მათ შორის 
Temporal Features:
- Time indices (Year, Month, Week, Quarter)
- Holiday flags (SuperBowl, LaborDay, Thanksgiving, Christmas)
- Lag features for sales (1, 2, 3, 4, 52 weeks back)

External Factor Features:
- Raw external variables (Temperature, Fuel_Price, CPI, Unemployment)
- Change features (differences and percentage changes)
- Lag features for external factors (1, 2, 4, 8, 12 weeks back)

Attention მექანიზმმა შეისწავლა თუ რომელი feature-ს რა დროს აქვს გავლენა. Gated Residual Network-ის წყალობით კი ტრანსფორმერი ავტომატურად გადაარჩევს მნიშვნელოვან feature-ებს.
TFT-ის Train დავყავით ფაზებად, ვამოდიფიცირებდით ჰიპერ პარამეტრებს hidden_size - რაც მეტია მით უკეთესად დაიჭერდა პატერნს, თუმცა იზრდება overfitting-ის რისკი. attention_head_size - სწორედ input-ის წონების მინიჭების მაკონტროლებელი პარამეტრია, თუკი 1-ია უბრალო attention-ია, თუკი 2-ია მაგალითად trend-და და სეზონურობას შეუძლია დააკვირდეს ცალ-ცალკე, ხოლო თუკი 4-ია მაშინ მრავალ რამეს შეუძლია დააკვირდეს ერთდროულად. hidden_continuous_size - უწყვეტი ცვლადების embedding-ის ზომა, რაც მეტია მით უკეთესად აღიწერებიან continouus variable-ები. dropout - რეგულარიზაცია, learning_rate, max_encoder_length რამდენით უკან ჩაიხედოს ისტორიაში, max_prediction_length რამდენით შორს იწინასწარმეტყველოს. საინტერესოა, რომ პრიველ 3 ფაზას იდენტური შედეგები ჰქონდა, იმის ხარჯზე, რომ early stopping მქონდა ჩართული და როცა მოდელი ახალს არაფერს არ სწავლობდა - ჩერდებოდა. თუმცა მეოთხე ფაზაში, როცა საგრძნობლად და აგრესიულად შევცვალე პარამეტრები, learning_rate 0.0005 დან 0.01-მდე ავწიეთ, hidden_size 64 დან 128 მდე, attention_head_size 2 დან 4 მდე, dropout 0.15 დან 0.3მდე, hidden_continuous_size 32-დან 64-მდე. თვითონ ტრეინინგის პარამეტრებში epoch 25 -> 50, gradient_clip_val 0.1 -> 1, early_stopping_patience 3 -> 15 weight_decay 1e-2 -> 1e-3, მოდელმა გაუძლო ამ ჰიპერპარამეტრებს რას კიდევ ერთხელ უსვამს ხაზს ამ ტრანსფორმერის მასშტაბურობას და განაგრძო სწავლა შედეგიანად. თუმცა დასაწყისში ჩვენ ავირჩიეთ მხოლოდ ის Store x Department წყვილები, რომლებსაც ჰქონდათ 30-ზე მეტი ჩანაწერი. როგორც აღმოჩნდა საკმაოდ ბევრი ჩანაწერი არ აკმაყოფილებდა ამ კრიტერიუმს, რის გამოც მოდელის საბმიშენის დაგენერირება ვერ შევძელით.
<img width="416" height="117" alt="image" src="https://github.com/user-attachments/assets/1740f3c4-1d6f-4a3f-aa71-4150d9c81f71" />


## DLinear

აკეთებს შემომავალი Input-ის დეკომპოზიციას ტრენდად და სეზონურობად, რომლებიც ერთ წრფივ შრეში გაივლიან და შეიკრიბებიან და მივიღებთ prediction-ს. როგორც ხედავთ, მოდელი საკმაოდ მარტივად გამოიყურება, თუმცა პრაქტიკა გვაჩვენებს, რომ ბევრი time series პრობლემის ეფექტურად გადაჭრაა შესაძლებელი მარტივად. DLinear პარამეტრად იღებს input_size ანუ Historical window length, ანუ რამდენად წარსულ მონაცემზე დაყრდნობით უნდა გააკეთოს წინასწარმეტყველება. h - ჰორიზონტი, ანუ მომავალში რამდენი პერიოდის წინასწარმეტყველება გინდა, max_steps - რამდენად დიდხანს უნდა დატრენინგდეს მოდელი, batch-size - თითოეულ ნაბიჯზე sample-ების რაოდენობა. Dlinear-ს შეუძლია აღმოაჩინოს ტრენდებისა და სეზონურობის განსხვავებული გავლენა პრობლემაზე, იყენებს, როგორც ვნახეთ, მინიმალურ პარამეტრებს რაც overfitting-ის რისკს ამცირებს, ასევე რადგან მხოლოდ და მხოლოდ წრფივ ოპერაციებს ვასრულებთ, ყველაფერი სრულდება სწრაფად.

ჩვენ გავტესტეთ შემდგომ ჰიპერპარამეტრების სივრცეზე:
1. Input Size: 26, 52, 78, 104 კვირა
2. Forecast Horizon: 24, 32, 40, 48 კვირა
3. Learning Rate: 0.0005, 0.001, 0.002, 0.005
4. Batch Size: 32, 64, 128, 256
5. Training Steps: 1000, 1500, 2000, 2500, 3000

თავდაპირველად ავიღეთ baseline პარამეტრები და ყოველ ეტაპზე სათითაოდ ვცვლიდით განსხვავებულით, საბოლოოდ ყველაზე მინიმალური wmae-ს შესაბამის პარამეტრს ვტოვებდით და ასე მივიღეთ შემდგომი კონფიგურაცია: <img width="192" height="129" alt="image" src="https://github.com/user-attachments/assets/9a6b2285-7dcc-4391-981f-8554aa30b4ca" />

DLinear-ს საკმაოდ მსგავსია NBEATS-თან შედარებით, რის გამოც გამიადვილდა გამოკვლევა, რადგან N-BEATS-ეტაპებს ემთხვეოდა. საბოლოოდ დავაგენერირეთ საბმიშენი და მივიღეთ wmae - 3475.31177


## N-BEATS

<img width="1519" height="965" alt="image" src="https://github.com/user-attachments/assets/47f73555-90cf-439e-a593-feaf75702cb7" />

N-BEATS - ში დავამატეთ წონები დღესასწაულებისთვის, დანარჩენი პროცესი, კერძოდ, ჰიპერპარამეტრების ტიუნინგი გავაკეთეთ DLinear-ის მსგავსად, ოღონდ აქ დამატებითი პარამეტრი გვაქვს, ბლოკების არქიტექტურა. მივიღეთ საუკეთესო კონფიგურაცია: Input Size: 52, Horizon: 40, Architecture: [5,5,5], Learning Rate: 0.001, Training Steps: 1500. შემდეგ დავაგენერირეთ საბმიშენი და მივიღეთ wmae - 2728.01864, რაც საუკეთესო შედეგია ჩვენს მოდელებს შორის. N-BEATS-მა იმუშავა კარგად რადგან მან აღმოაჩინა კომპლექსური იერარქიული პატერნები Walmart-ის დata-ში, რისი გამოყვანაც ხელით საკმაოდ რთული იქნებოდა. N-BEATS-ის დატრენინგება შედარებით ნელი იყო ვიდრე პირადპირ წრფივი DLINEAR. თუმცა მან უკეთესი შედეგი დადო, რადგან, როგორც ჩანს არაწრფივი კავშირების დადგენა უფრო ოპტიმალური აღმოჩნდა ვიდრე მისი წრფივად მიაახლოვება.
