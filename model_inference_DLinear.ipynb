{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMicOUpog41F9lx/dYzGRkT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2c1d4bcb28d64db0a96ae753c0eaa64c":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_3a9385e71ab241c9b9046403fa2714dd","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[32m‚†ô\u001b[0m Waiting for authorization\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†ô</span> Waiting for authorization\n</pre>\n"},"metadata":{}}]}},"3a9385e71ab241c9b9046403fa2714dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMh4BQsj90vK","executionInfo":{"status":"ok","timestamp":1754074463550,"user_tz":-240,"elapsed":4792,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}},"outputId":"84525872-f2fc-4715-c5d3-88cfdf86df89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.7)\n","Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.6.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: mlflow==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (8.2.1)\n","Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.2.1)\n","Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.18.0)\n","Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.4)\n","Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.1.45)\n","Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (6.0.2)\n","Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (4.25.8)\n","Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2022.7.1)\n","Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.32.3)\n","Requirement already satisfied: packaging<24 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (23.2)\n","Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (6.11.0)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.5.3)\n","Requirement already satisfied: alembic<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (1.16.4)\n","Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (6.1.3)\n","Requirement already satisfied: Flask<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.3.3)\n","Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (1.16.1)\n","Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (1.2.4)\n","Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.0.42)\n","Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (11.0.0)\n","Requirement already satisfied: shap<1,>=0.40 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.48.0)\n","Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.8.2)\n","Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.10.0)\n","Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (20.1.0)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.1.6)\n","Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n","Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.77)\n","Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n","Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\n","Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n","Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n","Requirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\n","Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n","Requirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.3)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\n","Requirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.8.0)\n","Requirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.3.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.40.0)\n","Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\n","Requirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.11)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2->mlflow==2.2.2) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic<2->mlflow==2.2.2) (4.14.1)\n","Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.3.0)\n","Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (2.11.7)\n","Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (2.10.1)\n","Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (3.3.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (0.9.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (1.17.0)\n","Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (2.5.0)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==2.2.2) (1.8.0)\n","Requirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.2.2) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.2.2) (2.2.0)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.2.2) (1.9.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==2.2.2) (4.0.12)\n","Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.11/dist-packages (from gunicorn<21->mlflow==2.2.2) (75.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.2.2) (3.23.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow==2.2.2) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (3.2.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.2.2) (3.4.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap<1,>=0.40->mlflow==2.2.2) (0.0.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap<1,>=0.40->mlflow==2.2.2) (0.60.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.2.2) (3.2.3)\n","Requirement already satisfied: botocore<1.41.0,>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.40.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\n","Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.13.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\n","Requirement already satisfied: graphql-core<3.2.7,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (3.2.6)\n","Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n","Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\n","Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.2.2) (5.0.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap<1,>=0.40->mlflow==2.2.2) (0.43.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.4.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.1.0)\n","Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n"]}],"source":["!pip install prophet dagshub scikit-learn pandas numpy mlflow==2.2.2"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import mlflow\n","import mlflow.pytorch\n","import dagshub\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","import warnings\n","import os\n","from datetime import datetime\n","\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"L-_b2ftF98mD","executionInfo":{"status":"ok","timestamp":1754074474408,"user_tz":-240,"elapsed":5775,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["os.environ['MLFLOW_TRACKING_USERNAME'] = 'nipkha21'\n","os.environ['MLFLOW_TRACKING_PASSWORD'] = '202fb8a4c58a90b0eb3598b1037498eb6fe9f593'\n","\n","dagshub.init(repo_owner='TomC333', repo_name='ml-walmart-recruiting', mlflow=True)\n","mlflow.set_tracking_uri('https://dagshub.com/TomC333/ml-walmart-recruiting.mlflow')\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229,"referenced_widgets":["2c1d4bcb28d64db0a96ae753c0eaa64c","3a9385e71ab241c9b9046403fa2714dd"]},"id":"TbUgByEq98op","executionInfo":{"status":"ok","timestamp":1754074482913,"user_tz":-240,"elapsed":5663,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}},"outputId":"8a8d1993-af75-4bc6-ccb1-fb384d8353d4"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c1d4bcb28d64db0a96ae753c0eaa64c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","Open the following link in your browser to authorize the client:\n","https://dagshub.com/login/oauth/authorize?state=399d758b-9269-47eb-8099-8a87e4820a6a&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=a7329495856ac8fa984fcd8d5af2b8012d1292f9776635da678deb8190dac315\n","\n","\n"]},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Accessing as nipkha21\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as nipkha21\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Initialized MLflow to track repo \u001b[32m\"TomC333/ml-walmart-recruiting\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"TomC333/ml-walmart-recruiting\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Repository TomC333/ml-walmart-recruiting initialized!\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository TomC333/ml-walmart-recruiting initialized!\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["class MovingAverage(nn.Module):\n","    \"\"\"Moving average block for DLinear\"\"\"\n","    def __init__(self, kernel_size, stride):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n","\n","    def forward(self, x):\n","        # Padding\n","        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        x = torch.cat([front, x, end], dim=1)\n","        x = self.avg(x.permute(0, 2, 1))\n","        x = x.permute(0, 2, 1)\n","        return x\n","\n","class SeriesDecomposition(nn.Module):\n","    \"\"\"Series decomposition block for DLinear\"\"\"\n","    def __init__(self, kernel_size):\n","        super().__init__()\n","        self.moving_avg = MovingAverage(kernel_size, stride=1)\n","\n","    def forward(self, x):\n","        moving_mean = self.moving_avg(x)\n","        residual = x - moving_mean\n","        return residual, moving_mean\n","\n","class DLinear(nn.Module):\n","    \"\"\"DLinear: Decomposition Linear for time series forecasting\"\"\"\n","    def __init__(self, seq_len, pred_len, enc_in=1, individual=False, kernel_size=25):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.pred_len = pred_len\n","        self.individual = individual\n","        self.channels = enc_in\n","\n","        # Decomposition\n","        self.decomposition = SeriesDecomposition(kernel_size)\n","\n","        if self.individual:\n","            self.Linear_Seasonal = nn.ModuleList([\n","                nn.Linear(self.seq_len, self.pred_len) for _ in range(self.channels)\n","            ])\n","            self.Linear_Trend = nn.ModuleList([\n","                nn.Linear(self.seq_len, self.pred_len) for _ in range(self.channels)\n","            ])\n","        else:\n","            self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n","            self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n","\n","    def forward(self, x):\n","        # x: [Batch, Input length, Channel]\n","        seasonal_init, trend_init = self.decomposition(x)\n","        seasonal_init = seasonal_init.permute(0, 2, 1)  # [Batch, Channel, Input length]\n","        trend_init = trend_init.permute(0, 2, 1)\n","\n","        if self.individual:\n","            seasonal_output = torch.zeros([x.size(0), x.size(2), self.pred_len], dtype=x.dtype, device=x.device)\n","            trend_output = torch.zeros([x.size(0), x.size(2), self.pred_len], dtype=x.dtype, device=x.device)\n","\n","            for i in range(self.channels):\n","                seasonal_output[:, i, :] = self.Linear_Seasonal[i](seasonal_init[:, i, :])\n","                trend_output[:, i, :] = self.Linear_Trend[i](trend_init[:, i, :])\n","        else:\n","            seasonal_output = self.Linear_Seasonal(seasonal_init)\n","            trend_output = self.Linear_Trend(trend_init)\n","\n","        x = seasonal_output + trend_output\n","        return x.permute(0, 2, 1)  # [Batch, Output length, Channel]"],"metadata":{"id":"S8xcoKnW98rm","executionInfo":{"status":"ok","timestamp":1754074920936,"user_tz":-240,"elapsed":11,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def load_and_prepare_test_data():\n","    \"\"\"Load and prepare test data with same preprocessing as training\"\"\"\n","    print(\"Loading test data...\")\n","\n","    # Load datasets\n","    features_data = pd.read_csv('features.csv')\n","    train_data = pd.read_csv('train.csv')  # Need for feature engineering\n","    test_data = pd.read_csv('test.csv')\n","    stores = pd.read_csv('stores.csv')\n","\n","    print(f\"Test data shape: {test_data.shape}\")\n","\n","    # Combine train and test for consistent feature engineering\n","    train_data['is_test'] = False\n","    test_data['is_test'] = True\n","    test_data['Weekly_Sales'] = np.nan  # Placeholder for test set\n","\n","    combined_data = pd.concat([train_data, test_data], ignore_index=True)\n","\n","    # Merge with features and stores\n","    df = combined_data.merge(features_data, on=['Store', 'Date'], how='inner').merge(stores, on=['Store'], how='inner')\n","\n","    # Handle duplicate IsHoliday columns\n","    if 'IsHoliday_y' in df.columns:\n","        df.drop(['IsHoliday_y'], axis=1, inplace=True)\n","        df.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)\n","\n","    print(f\"Combined dataset shape: {df.shape}\")\n","    return df\n","\n","def apply_feature_engineering(df):\n","    \"\"\"Apply same feature engineering as training\"\"\"\n","    print(\"Applying feature engineering...\")\n","\n","    # Convert Date to datetime and sort\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values(by=['Store', 'Dept', 'Date'])\n","\n","    # Remove negative sales from training portion only\n","    train_mask = df['is_test'] == False\n","    negative_sales_mask = df['Weekly_Sales'] < 0\n","    df = df[~(train_mask & negative_sales_mask)]\n","\n","    # Create lag features for Weekly_Sales\n","    lags = [1, 2, 3, 4, 52]\n","    for lag in lags:\n","        df[f'Weekly_Sales_lag_{lag}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n","\n","    # Create Sales_diff1 (but won't use as per your note)\n","    df['Sales_diff1'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].diff()\n","\n","    # Create time features\n","    df['Year'] = df['Date'].dt.year\n","    df['Month'] = df['Date'].dt.month\n","    df['Week'] = df['Date'].dt.isocalendar().week\n","    df['Quarter'] = df['Date'].dt.quarter\n","\n","    # Create change features for external variables\n","    for col in ['Fuel_Price', 'CPI', 'Unemployment']:\n","        if col in df.columns:\n","            df[f'{col}_change'] = df.groupby(['Store', 'Dept'])[col].diff()\n","            df[f'{col}_pct_change'] = df.groupby(['Store', 'Dept'])[col].pct_change()\n","\n","    # Create lag features for external variables\n","    for lag in [1, 2, 4, 8, 12]:\n","        for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']:\n","            if col in df.columns:\n","                df[f'{col}_lag_{lag}'] = df.groupby(['Store', 'Dept'])[col].shift(lag)\n","\n","    # Create outlier features (using same thresholds as training)\n","    df['is_outlier'] = ((df['Weekly_Sales'] < -25108.67) | (df['Weekly_Sales'] > 847494.61)).astype(int)\n","    outlier_propensity_store = df[df['is_test'] == False].groupby('Store')['is_outlier'].mean()\n","    outlier_propensity_dept = df[df['is_test'] == False].groupby('Dept')['is_outlier'].mean()\n","    df['store_outlier_propensity'] = df['Store'].map(outlier_propensity_store)\n","    df['dept_outlier_propensity'] = df['Dept'].map(outlier_propensity_dept)\n","    df['is_outlier_lag1'] = df.groupby(['Store', 'Dept'])['is_outlier'].shift(1)\n","    df['outlier_count_last_4weeks'] = df.groupby(['Store', 'Dept'])['is_outlier'].rolling(4).sum().shift(1).reset_index(level=[0,1], drop=True)\n","    df.drop('is_outlier', axis=1, inplace=True)\n","\n","    # Create holiday flags\n","    df['Is_SuperBowl'] = np.where(df['Date'].isin(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08']), 1, 0)\n","    df['Is_LaborDay'] = np.where(df['Date'].isin(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06']), 1, 0)\n","    df['Is_Thanksgiving'] = np.where(df['Date'].isin(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29']), 1, 0)\n","    df['Is_Christmas'] = np.where(df['Date'].isin(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']), 1, 0)\n","\n","    print(\"Feature engineering completed\")\n","    return df\n","\n","def apply_data_cleaning(df):\n","    \"\"\"Apply same data cleaning as training\"\"\"\n","    print(\"Applying data cleaning...\")\n","\n","    # Drop markdown columns\n","    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n","    for col in markdown_cols:\n","        if col in df.columns:\n","            df = df.drop(columns=[col])\n","\n","    # Handle lag features\n","    lag_cols = [col for col in df.columns if 'lag' in col.lower()]\n","    for col in lag_cols:\n","        df[col] = df.groupby(['Store', 'Dept'])[col].ffill().bfill()\n","        df[col] = df[col].fillna(df[col].median())\n","\n","    # Handle external factor derived features\n","    external_derived_cols = [col for col in df.columns if any(x in col for x in ['change', 'pct_change'])\n","                            and any(y in col for y in ['Fuel_Price', 'CPI', 'Unemployment', 'Temperature'])]\n","\n","    for col in external_derived_cols:\n","        if col in df.columns:\n","            df[col] = df.groupby(['Store', 'Dept'])[col].ffill()\n","            df[col] = df[col].fillna(0)\n","\n","    # Handle outlier features\n","    outlier_cols = [col for col in df.columns if 'outlier' in col.lower()]\n","    for col in outlier_cols:\n","        if col in df.columns:\n","            df[col] = df[col].fillna(0)\n","\n","    # Final cleanup\n","    remaining_missing = df.isnull().sum()\n","    remaining_missing = remaining_missing[remaining_missing > 0]\n","\n","    for col in remaining_missing.index:\n","        if df[col].dtype in ['int64', 'float64']:\n","            df[col] = df.groupby(['Store', 'Dept'])[col].ffill().bfill()\n","            df[col] = df[col].fillna(df[col].median())\n","        else:\n","            df[col] = df.groupby(['Store', 'Dept'])[col].ffill()\n","            df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown')\n","\n","    print(\"‚úÖ Data cleaning completed\")\n","    return df"],"metadata":{"id":"oRZBWrwM98uK","executionInfo":{"status":"ok","timestamp":1754076293785,"user_tz":-240,"elapsed":18,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def load_best_model_from_run(run_id):\n","    \"\"\"Load the best DLinear model from specific MLflow run using actual logged artifacts\"\"\"\n","    print(f\"Loading model from run: {run_id}\")\n","\n","    try:\n","        # Load the model using MLmodel artifact\n","        model_uri = f\"runs:/{run_id}/best_dlinear_model\"\n","        loaded_model = mlflow.pytorch.load_model(model_uri)\n","        print(f\"‚úÖ Model loaded from MLmodel artifact\")\n","\n","        # Load model metadata from pickle file\n","        import tempfile\n","        import os\n","        with tempfile.TemporaryDirectory() as temp_dir:\n","            try:\n","                mlflow.artifacts.download_artifacts(f\"runs:/{run_id}/best_dlinear_metadata.pkl\", dst_path=temp_dir)\n","                with open(os.path.join(temp_dir, \"best_dlinear_metadata.pkl\"), 'rb') as f:\n","                    model_metadata = pickle.load(f)\n","                print(f\"‚úÖ Loaded model metadata from pickle file\")\n","            except Exception as e:\n","                print(f\"Could not load metadata pickle: {e}\")\n","                # Use defaults if metadata not available\n","                model_metadata = {\n","                    'seq_len': 20,\n","                    'pred_len': 10,\n","                    'n_features': 1,\n","                    'phase_name': 'Unknown',\n","                    'features': []\n","                }\n","\n","        # Load normalization stats - try multiple possible names\n","        normalization_stats = None\n","        possible_norm_files = [\n","            'dlinear_normalization_stats.pkl',\n","            'normalization_stats.pkl',\n","            'norm_stats.pkl'\n","        ]\n","\n","        for norm_file in possible_norm_files:\n","            try:\n","                with tempfile.TemporaryDirectory() as temp_dir:\n","                    mlflow.artifacts.download_artifacts(f\"runs:/{run_id}/{norm_file}\", dst_path=temp_dir)\n","                    with open(os.path.join(temp_dir, norm_file), 'rb') as f:\n","                        normalization_stats = pickle.load(f)\n","                    print(f\"‚úÖ Loaded normalization stats from {norm_file}\")\n","                    break\n","            except:\n","                continue\n","\n","        if normalization_stats is None:\n","            print(\"‚ö†Ô∏è Could not load normalization stats, using defaults\")\n","            normalization_stats = {'mean': 0, 'std': 1}\n","\n","        # Create preprocessing config from available information\n","        preprocessing_config = {\n","            'seq_len': model_metadata.get('seq_len', 20),\n","            'pred_len': model_metadata.get('pred_len', 10),\n","            'features': model_metadata.get('features', []),\n","            'normalization_stats': normalization_stats\n","        }\n","\n","        print(f\"‚úÖ Model loaded successfully!\")\n","        print(f\"Model phase: {model_metadata.get('phase_name', 'Unknown')}\")\n","        print(f\"Features: {len(model_metadata.get('features', []))}\")\n","        print(f\"Seq len: {model_metadata.get('seq_len', 20)}, Pred len: {model_metadata.get('pred_len', 10)}\")\n","\n","        return loaded_model, model_metadata, normalization_stats, preprocessing_config\n","\n","    except Exception as e:\n","        print(f\"Failed to load from run {run_id}: {e}\")\n","        raise Exception(f\"Could not load model from run {run_id}\")\n","\n","def load_best_model_with_run_id(run_id=None):\n","    \"\"\"Load model with provided run_id\"\"\"\n","\n","    if run_id:\n","        # Use provided run_id\n","        return load_best_model_from_run(run_id)\n","    else:\n","        raise Exception(\"run_id is required - no fallback available\")\n","\n","def prepare_inference_data(df, metadata, norm_stats):\n","    \"\"\"Prepare data for inference - optimized version\"\"\"\n","    print(\"Preparing inference data...\")\n","\n","    # Split train and test\n","    train_df = df[df['is_test'] == False].copy()\n","    test_df = df[df['is_test'] == True].copy()\n","\n","    # Apply normalization to training data\n","    if 'Weekly_Sales_normalized' not in train_df.columns:\n","        train_df['Weekly_Sales_normalized'] = (train_df['Weekly_Sales'] - norm_stats['mean']) / norm_stats['std']\n","\n","    seq_len = metadata['seq_len']\n","    feature_cols = metadata.get('features', [])\n","\n","    # Get unique store-dept combinations (not dates)\n","    store_dept_groups = test_df[['Store', 'Dept']].drop_duplicates()\n","    print(f\"Processing {len(store_dept_groups)} store-dept combinations...\")\n","\n","    # Pre-group train data for faster lookup\n","    train_grouped = train_df.groupby(['Store', 'Dept'])\n","\n","    inference_data = []\n","    processed = 0\n","\n","    for _, row in store_dept_groups.iterrows():\n","        store, dept = row['Store'], row['Dept']\n","\n","        try:\n","            # Get historical data for this store-dept\n","            hist_data = train_grouped.get_group((store, dept)).sort_values('Date')\n","        except KeyError:\n","            # No historical data - use global median\n","            print(f\"No historical data for Store {store}, Dept {dept} - using global median\")\n","            hist_data = None\n","\n","        # Get all test dates for this store-dept\n","        test_dates = test_df[(test_df['Store'] == store) & (test_df['Dept'] == dept)]['Date'].values\n","\n","        if hist_data is not None and len(hist_data) >= seq_len:\n","            # Use historical data\n","            input_data = hist_data.tail(seq_len)\n","            input_seq = input_data['Weekly_Sales_normalized'].values.astype(np.float32)\n","\n","            # Handle features\n","            if feature_cols:\n","                feature_seq = input_data[feature_cols].values.astype(np.float32)\n","                combined_input = np.column_stack([input_seq.reshape(-1, 1), feature_seq])\n","            else:\n","                combined_input = input_seq.reshape(-1, 1)\n","\n","            # Quick NaN check and fix\n","            if np.any(np.isnan(combined_input)):\n","                combined_input = np.nan_to_num(combined_input, nan=0.0)\n","        else:\n","            # No historical data or not enough - create dummy input with global median\n","            global_median_normalized = 0.0  # Since we're using normalized data\n","            if feature_cols:\n","                dummy_features = np.zeros((seq_len, len(feature_cols)), dtype=np.float32)\n","                combined_input = np.column_stack([np.full((seq_len, 1), global_median_normalized, dtype=np.float32), dummy_features])\n","            else:\n","                combined_input = np.full((seq_len, 1), global_median_normalized, dtype=np.float32)\n","\n","        # Create one sample per test date\n","        for test_date in test_dates:\n","            inference_data.append({\n","                'input': combined_input,\n","                'store': store,\n","                'dept': dept,\n","                'date': test_date\n","            })\n","\n","        processed += 1\n","        if processed % 100 == 0:\n","            print(f\"Processed {processed}/{len(store_dept_groups)} combinations...\")\n","\n","    print(f\"Prepared {len(inference_data)} inference samples\")\n","    return inference_data\n","\n","def generate_predictions(model, inference_data, metadata, norm_stats, device):\n","    \"\"\"Generate predictions using the loaded model\"\"\"\n","    print(\"Generating predictions...\")\n","\n","    model = model.to(device)\n","    model.eval()\n","\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for sample in inference_data:\n","            input_tensor = torch.FloatTensor(sample['input']).unsqueeze(0).to(device)  # Add batch dimension\n","\n","            # Generate prediction\n","            pred_tensor = model(input_tensor)  # [1, pred_len, n_channels]\n","            pred_normalized = pred_tensor[0, :, 0].cpu().numpy()  # Take first channel, remove batch dim\n","\n","            # Denormalize predictions\n","            pred_actual = pred_normalized * norm_stats['std'] + norm_stats['mean']\n","\n","            # Ensure non-negative predictions\n","            pred_actual = np.maximum(pred_actual, 0)\n","\n","            # Store prediction (we'll use the first prediction for weekly forecast)\n","            predictions.append({\n","                'Store': sample['store'],\n","                'Dept': sample['dept'],\n","                'Date': sample['date'],\n","                'Weekly_Sales': pred_actual[0]  # Use first prediction\n","            })\n","\n","    print(f\"Generated {len(predictions)} predictions\")\n","    return predictions"],"metadata":{"id":"GOp0OEJ798w7","executionInfo":{"status":"ok","timestamp":1754077658378,"user_tz":-240,"elapsed":21,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def create_kaggle_submission(predictions):\n","    \"\"\"Create Kaggle submission file\"\"\"\n","    print(\"Creating Kaggle submission file...\")\n","\n","    # Convert to DataFrame\n","    pred_df = pd.DataFrame(predictions)\n","\n","    # Create Id column in the required format: Store_Dept_Date\n","    pred_df['Id'] = pred_df['Store'].astype(str) + '_' + pred_df['Dept'].astype(str) + '_' + pred_df['Date'].astype(str)\n","\n","    # Create submission dataframe\n","    submission = pred_df[['Id', 'Weekly_Sales']].copy()\n","    submission = submission.rename(columns={'Weekly_Sales': 'Weekly_Sales'})\n","\n","    # Sort by Id for consistency\n","    submission = submission.sort_values('Id')\n","\n","    # Save submission file\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    submission_filename = f'dlinear_submission_{timestamp}.csv'\n","    submission.to_csv(submission_filename, index=False)\n","\n","    print(f\"‚úÖ Submission file created: {submission_filename}\")\n","    print(f\"Submission shape: {submission.shape}\")\n","    print(f\"Sample predictions:\")\n","    print(submission.head(10))\n","\n","    return submission, submission_filename"],"metadata":{"id":"PMb_p58F98zo","executionInfo":{"status":"ok","timestamp":1754074927637,"user_tz":-240,"elapsed":5,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(\"=\"*60)\n","print(\"DLINEAR MODEL INFERENCE PIPELINE\")\n","print(\"=\"*60)\n","\n","run_id = \"2f633bd2cfaa4473b9b27348279475e7\"\n","\n","if run_id:\n","    print(f\"Using provided run_id: {run_id}\")\n","else:\n","    print(\"No run_id provided - will search for best model\")\n","\n","# Start MLflow run for inference\n","mlflow.set_experiment(\"DLinear_Inference\")\n","\n","with mlflow.start_run(run_name=f\"DLinear_Inference_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n","\n","    # Initialize variables\n","    model = None\n","    metadata = {}\n","    norm_stats = {'mean': 0, 'std': 1}\n","    preprocessing_config = {}\n","    df = None\n","    inference_data = []\n","    predictions = []\n","\n","    # 1. Load best model from run_id\n","    try:\n","        model, metadata, norm_stats, preprocessing_config = load_best_model_with_run_id(run_id)\n","\n","        mlflow.log_param(\"source_run_id\", run_id if run_id else \"registry\")\n","        mlflow.log_param(\"model_phase\", metadata.get('phase_name', 'Unknown'))\n","        mlflow.log_param(\"n_features\", metadata.get('n_features', 1))\n","        mlflow.log_param(\"seq_len\", metadata.get('seq_len', 20))\n","        mlflow.log_param(\"pred_len\", metadata.get('pred_len', 10))\n","        mlflow.log_param(\"device\", str(device))\n","\n","        print(\"‚úÖ Model loaded successfully!\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Failed to load model: {e}\")\n","        mlflow.log_param(\"error_model_loading\", str(e))\n","        # Don't exit, continue with error\n","\n","    # 2. Load and prepare test data\n","    if model is not None:\n","        try:\n","            df = load_and_prepare_test_data()\n","            mlflow.log_param(\"total_data_shape\", str(df.shape))\n","\n","            # Apply feature engineering\n","            df = apply_feature_engineering(df)\n","            mlflow.log_param(\"post_feature_engineering_shape\", str(df.shape))\n","\n","            # Apply data cleaning\n","            df = apply_data_cleaning(df)\n","            mlflow.log_param(\"final_clean_shape\", str(df.shape))\n","\n","            # Count test samples\n","            test_samples = len(df[df['is_test'] == True])\n","            mlflow.log_param(\"test_samples\", test_samples)\n","            print(f\"Test samples to predict: {test_samples}\")\n","\n","            print(\"‚úÖ Data preparation completed!\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Failed to prepare data: {e}\")\n","            mlflow.log_param(\"error_data_prep\", str(e))\n","            df = None\n","\n","    # 3. Prepare inference data\n","    if df is not None:\n","        try:\n","            inference_data = prepare_inference_data(df, metadata, norm_stats)\n","            mlflow.log_param(\"inference_samples\", len(inference_data))\n","\n","            if len(inference_data) == 0:\n","                print(\"‚ùå No inference samples prepared\")\n","                mlflow.log_param(\"error_inference_prep\", \"No inference samples prepared\")\n","            else:\n","                print(\"‚úÖ Inference data prepared!\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Failed to prepare inference data: {e}\")\n","            mlflow.log_param(\"error_inference_prep\", str(e))\n","            inference_data = []\n","\n","    # 4. Generate predictions\n","    if len(inference_data) > 0:\n","        try:\n","            predictions = generate_predictions(model, inference_data, metadata, norm_stats, device)\n","            mlflow.log_param(\"predictions_generated\", len(predictions))\n","\n","            if len(predictions) > 0:\n","                # Log prediction statistics\n","                pred_values = [p['Weekly_Sales'] for p in predictions]\n","                mlflow.log_metric(\"pred_mean\", np.mean(pred_values))\n","                mlflow.log_metric(\"pred_std\", np.std(pred_values))\n","                mlflow.log_metric(\"pred_min\", np.min(pred_values))\n","                mlflow.log_metric(\"pred_max\", np.max(pred_values))\n","\n","                print(f\"‚úÖ Predictions generated!\")\n","                print(f\"Prediction statistics:\")\n","                print(f\"  Mean: {np.mean(pred_values):.2f}\")\n","                print(f\"  Std: {np.std(pred_values):.2f}\")\n","                print(f\"  Min: {np.min(pred_values):.2f}\")\n","                print(f\"  Max: {np.max(pred_values):.2f}\")\n","            else:\n","                print(\"‚ùå No predictions generated\")\n","                mlflow.log_param(\"error_prediction\", \"No predictions generated\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Failed to generate predictions: {e}\")\n","            mlflow.log_param(\"error_prediction\", str(e))\n","            predictions = []\n","\n","    # 5. Create Kaggle submission\n","    if len(predictions) > 0:\n","        try:\n","            submission, submission_filename = create_kaggle_submission(predictions)\n","            mlflow.log_param(\"submission_filename\", submission_filename)\n","            mlflow.log_param(\"submission_shape\", str(submission.shape))\n","\n","            # Log submission file as artifact\n","            mlflow.log_artifact(submission_filename)\n","\n","            # Create summary statistics\n","            pred_values = [p['Weekly_Sales'] for p in predictions]\n","            summary_stats = {\n","                'total_predictions': len(submission),\n","                'unique_stores': len(set(p['Store'] for p in predictions)),\n","                'unique_departments': len(set(p['Dept'] for p in predictions)),\n","                'prediction_mean': np.mean(pred_values),\n","                'prediction_std': np.std(pred_values),\n","                'model_phase': metadata.get('phase_name', 'Unknown'),\n","                'features_used': len(metadata.get('features', []))\n","            }\n","\n","            # Log summary as metrics (not params to avoid collisions)\n","            mlflow.log_metric(\"total_predictions\", summary_stats['total_predictions'])\n","            mlflow.log_metric(\"unique_stores\", summary_stats['unique_stores'])\n","            mlflow.log_metric(\"unique_departments\", summary_stats['unique_departments'])\n","            mlflow.log_metric(\"features_used\", summary_stats['features_used'])\n","\n","            # Save summary\n","            with open('inference_summary.txt', 'w') as f:\n","                f.write(\"DLinear Model Inference Summary\\n\")\n","                f.write(\"=\"*40 + \"\\n\\n\")\n","                for key, value in summary_stats.items():\n","                    f.write(f\"{key}: {value}\\n\")\n","\n","            mlflow.log_artifact('inference_summary.txt')\n","\n","            print(f\"\\nüéâ Inference completed successfully!\")\n","            print(f\"üìä Submission file: {submission_filename}\")\n","            print(f\"üìà Total predictions: {len(submission)}\")\n","            print(f\"üè™ Unique stores: {summary_stats['unique_stores']}\")\n","            print(f\"üè∑Ô∏è Unique departments: {summary_stats['unique_departments']}\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Failed to create submission: {e}\")\n","            mlflow.log_param(\"error_submission\", str(e))\n","\n","    else:\n","        print(\"‚ùå Cannot create submission - no predictions available\")\n","        mlflow.log_param(\"final_status\", \"Failed - no predictions\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"INFERENCE PIPELINE COMPLETED\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5v8e4LZi982i","executionInfo":{"status":"ok","timestamp":1754077946939,"user_tz":-240,"elapsed":283200,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}},"outputId":"f280385d-40da-4911-ad7f-a8a0af15fd0f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","DLINEAR MODEL INFERENCE PIPELINE\n","============================================================\n","Using provided run_id: 2f633bd2cfaa4473b9b27348279475e7\n","Loading model from run: 2f633bd2cfaa4473b9b27348279475e7\n","‚úÖ Model loaded from MLmodel artifact\n","‚úÖ Loaded model metadata from pickle file\n","‚ö†Ô∏è Could not load normalization stats, using defaults\n","‚úÖ Model loaded successfully!\n","Model phase: Phase3_Holidays\n","Features: 8\n","Seq len: 20, Pred len: 10\n","‚úÖ Model loaded successfully!\n","Loading test data...\n","Test data shape: (115064, 4)\n","Combined dataset shape: (536634, 17)\n","Applying feature engineering...\n","Feature engineering completed\n","Applying data cleaning...\n","‚úÖ Data cleaning completed\n","Test samples to predict: 115064\n","‚úÖ Data preparation completed!\n","Preparing inference data...\n","Processing 3169 store-dept combinations...\n","Processed 100/3169 combinations...\n","Processed 200/3169 combinations...\n","Processed 300/3169 combinations...\n","No historical data for Store 5, Dept 99 - using global median\n","Processed 400/3169 combinations...\n","Processed 500/3169 combinations...\n","Processed 600/3169 combinations...\n","No historical data for Store 9, Dept 99 - using global median\n","Processed 700/3169 combinations...\n","No historical data for Store 10, Dept 99 - using global median\n","Processed 800/3169 combinations...\n","Processed 900/3169 combinations...\n","Processed 1000/3169 combinations...\n","No historical data for Store 15, Dept 37 - using global median\n","Processed 1100/3169 combinations...\n","Processed 1200/3169 combinations...\n","No historical data for Store 18, Dept 43 - using global median\n","Processed 1300/3169 combinations...\n","Processed 1400/3169 combinations...\n","Processed 1500/3169 combinations...\n","Processed 1600/3169 combinations...\n","Processed 1700/3169 combinations...\n","No historical data for Store 24, Dept 43 - using global median\n","Processed 1800/3169 combinations...\n","No historical data for Store 25, Dept 99 - using global median\n","Processed 1900/3169 combinations...\n","Processed 2000/3169 combinations...\n","Processed 2100/3169 combinations...\n","Processed 2200/3169 combinations...\n","Processed 2300/3169 combinations...\n","Processed 2400/3169 combinations...\n","No historical data for Store 34, Dept 39 - using global median\n","No historical data for Store 35, Dept 19 - using global median\n","Processed 2500/3169 combinations...\n","No historical data for Store 36, Dept 30 - using global median\n","Processed 2600/3169 combinations...\n","No historical data for Store 37, Dept 29 - using global median\n","Processed 2700/3169 combinations...\n","Processed 2800/3169 combinations...\n","Processed 2900/3169 combinations...\n","No historical data for Store 42, Dept 30 - using global median\n","Processed 3000/3169 combinations...\n","No historical data for Store 43, Dept 55 - using global median\n","Processed 3100/3169 combinations...\n","No historical data for Store 45, Dept 39 - using global median\n","Prepared 115064 inference samples\n","‚úÖ Inference data prepared!\n","Generating predictions...\n","Generated 115064 predictions\n","‚úÖ Predictions generated!\n","Prediction statistics:\n","  Mean: 7.80\n","  Std: 23.99\n","  Min: 0.00\n","  Max: 380.14\n","Creating Kaggle submission file...\n","‚úÖ Submission file created: dlinear_submission_20250801_195221.csv\n","Submission shape: (115064, 2)\n","Sample predictions:\n","                     Id  Weekly_Sales\n","24245  10_10_2012-11-02           0.0\n","24246  10_10_2012-11-09           0.0\n","24247  10_10_2012-11-16           0.0\n","24248  10_10_2012-11-23           0.0\n","24249  10_10_2012-11-30           0.0\n","24250  10_10_2012-12-07           0.0\n","24251  10_10_2012-12-14           0.0\n","24252  10_10_2012-12-21           0.0\n","24253  10_10_2012-12-28           0.0\n","24254  10_10_2013-01-04           0.0\n","\n","üéâ Inference completed successfully!\n","üìä Submission file: dlinear_submission_20250801_195221.csv\n","üìà Total predictions: 115064\n","üè™ Unique stores: 45\n","üè∑Ô∏è Unique departments: 81\n","\n","============================================================\n","INFERENCE PIPELINE COMPLETED\n","============================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"t8mNbQ6x9847"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6ZBJtTlR987f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qZhsJDr898-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i2xp8VBn99Aq"},"execution_count":null,"outputs":[]}]}