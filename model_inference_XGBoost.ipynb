{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dda1205-ba1c-4ccf-87ad-0fbaeb9f5511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"TomC333/ml-walmart-recruiting\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"TomC333/ml-walmart-recruiting\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository TomC333/ml-walmart-recruiting initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository TomC333/ml-walmart-recruiting initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf4a992871e410db5480b67182d6a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/01r863q91sz9mhh3gsvm_d640000gn/T/ipykernel_20802/415137199.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([train_df, test_merged], sort=False).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Id  Weekly_Sales_x  Weekly_Sales_y\n",
      "0         1_1_2012-11-02               0    39319.292969\n",
      "1         1_1_2012-11-09               0    30041.716797\n",
      "2         1_1_2012-11-16               0    29517.824219\n",
      "3         1_1_2012-11-23               0    16485.263672\n",
      "4         1_1_2012-11-30               0    20329.417969\n",
      "...                  ...             ...             ...\n",
      "115059  45_98_2013-06-28               0      789.463318\n",
      "115060  45_98_2013-07-05               0      813.141113\n",
      "115061  45_98_2013-07-12               0      668.398193\n",
      "115062  45_98_2013-07-19               0      640.965027\n",
      "115063  45_98_2013-07-26               0      694.878174\n",
      "\n",
      "[115064 rows x 3 columns]\n",
      "Final submission saved.\n",
      "Inference complete. submission.csv saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import dagshub\n",
    "\n",
    "# Initialize DagsHub + MLflow\n",
    "dagshub.init(repo_owner='TomC333', repo_name='ml-walmart-recruiting', mlflow=True)\n",
    "model_name = \"Best_XGBoost_Model\"\n",
    "model_uri = f\"models:/{model_name}/latest\"\n",
    "\n",
    "# Load the model from MLflow registry\n",
    "model = mlflow.xgboost.load_model(model_uri)\n",
    "\n",
    "# Load train and test merged data (with sales)\n",
    "train_df = pd.read_csv('train_merged_full.csv', parse_dates=['Date'])\n",
    "test_df = pd.read_csv('data/test.csv', parse_dates=['Date'])\n",
    "features_df = pd.read_csv('data/features.csv', parse_dates=['Date'])\n",
    "stores_df = pd.read_csv('data/stores.csv')\n",
    "\n",
    "# Merge test data with features and stores\n",
    "test_merged = test_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
    "test_merged = test_merged.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "if 'IsHoliday_y' in test_merged.columns:\n",
    "    test_merged.drop(['IsHoliday_y'], axis=1, inplace=True)\n",
    "    test_merged.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)\n",
    "\n",
    "# Basic date features\n",
    "test_merged['Month'] = test_merged['Date'].dt.month\n",
    "test_merged['Year'] = test_merged['Date'].dt.year\n",
    "test_merged['Week'] = test_merged['Date'].dt.isocalendar().week.astype(int)\n",
    "test_merged['Quarter'] = test_merged['Date'].dt.quarter\n",
    "\n",
    "# Sort train and test data\n",
    "train_df = train_df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
    "test_merged = test_merged.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Initialize lag columns in test set as NA\n",
    "for lag in [1, 2, 3, 4, 52]:\n",
    "    test_merged[f'Sales_Lag_{lag}'] = pd.NA\n",
    "\n",
    "# Calculate median sales per Store-Dept for fallback lag filling\n",
    "median_sales = train_df.groupby(['Store', 'Dept'])['Weekly_Sales'].median()\n",
    "\n",
    "# Combine train + test for iterative prediction and lag update\n",
    "combined = pd.concat([train_df, test_merged], sort=False).reset_index(drop=True)\n",
    "\n",
    "all_predictions = []\n",
    "unique_dates = sorted(test_merged['Date'].unique())\n",
    "\n",
    "features = [\n",
    "    'Store', 'Dept', 'IsHoliday', 'Month', 'Year', 'Week', 'Quarter',\n",
    "    'Temperature', 'Fuel_Price', 'Size', 'Type',\n",
    "    'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas',\n",
    "    'Sales_Lag_1', 'Sales_Lag_2', 'Sales_Lag_3', 'Sales_Lag_4', 'Sales_Lag_52'\n",
    "]\n",
    "\n",
    "for current_date in unique_dates:\n",
    "    # Filter test rows for current_date without predicted sales\n",
    "    test_current = combined[\n",
    "        (combined['Date'] == current_date) & \n",
    "        (combined['Weekly_Sales'].isna())\n",
    "    ].copy()\n",
    "    \n",
    "    # Merge lag features for each lag from previous weeks' sales\n",
    "    for lag in [1, 2, 3, 4, 52]:\n",
    "        lag_date = current_date - pd.Timedelta(weeks=lag)\n",
    "        lag_vals = combined[\n",
    "            combined['Date'] == lag_date\n",
    "        ][['Store', 'Dept', 'Weekly_Sales']].copy()\n",
    "        \n",
    "        lag_vals.rename(columns={'Weekly_Sales': f'Sales_Lag_{lag}'}, inplace=True)\n",
    "        \n",
    "        test_current = test_current.merge(\n",
    "            lag_vals, on=['Store', 'Dept'], how='left', suffixes=('', f'_lag{lag}')\n",
    "        ).copy()\n",
    "        \n",
    "        lag_col_with_suffix = f'Sales_Lag_{lag}_lag{lag}'\n",
    "        if lag_col_with_suffix in test_current.columns:\n",
    "            test_current[f'Sales_Lag_{lag}'] = test_current[lag_col_with_suffix]\n",
    "            test_current.drop(columns=[lag_col_with_suffix], inplace=True)\n",
    "\n",
    "    # Fill missing lag values using median fallback per Store-Dept\n",
    "    for lag in [1, 2, 3, 4, 52]:\n",
    "        test_current[f'Sales_Lag_{lag}'] = test_current[f'Sales_Lag_{lag}'].fillna(\n",
    "            test_current.apply(lambda row: median_sales.get((row['Store'], row['Dept']), 0), axis=1)\n",
    "        )\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    X_pred = test_current[features].copy()\n",
    "    \n",
    "    # Encode 'Type' as category codes\n",
    "    X_pred['Type'] = X_pred['Type'].astype('category').cat.codes\n",
    "\n",
    "    # Create DMatrix for XGBoost\n",
    "    dmatrix_pred = xgb.DMatrix(X_pred)\n",
    "    \n",
    "    # Predict sales and clip negatives to zero\n",
    "    preds = model.predict(dmatrix_pred).clip(min=0)\n",
    "    \n",
    "    # Store predictions back in combined dataframe\n",
    "    combined.loc[test_current.index, 'Weekly_Sales'] = preds\n",
    "    \n",
    "    # Save predictions for this date\n",
    "    all_predictions.append(test_current.assign(Weekly_Sales=preds))\n",
    "\n",
    "# Combine all date predictions into one DataFrame\n",
    "predictions_df = pd.concat(all_predictions).sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Create 'Id' in predictions_df to match submission Id format\n",
    "predictions_df['Id'] = (\n",
    "    predictions_df['Store'].astype(str) + '_' +\n",
    "    predictions_df['Dept'].astype(str) + '_' +\n",
    "    predictions_df['Date'].dt.strftime('%Y-%m-%d')\n",
    ")\n",
    "\n",
    "# Load submission template\n",
    "submission = pd.read_csv('data/sampleSubmission.csv')\n",
    "\n",
    "# Merge predictions on 'Id'\n",
    "submission = submission.merge(\n",
    "    predictions_df[['Id', 'Weekly_Sales']],\n",
    "    on='Id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(submission)\n",
    "\n",
    "submission['Weekly_Sales'] = submission['Weekly_Sales_y'].fillna(0)\n",
    "\n",
    "# Drop the old columns\n",
    "submission = submission.drop(columns=['Weekly_Sales_x', 'Weekly_Sales_y'])\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Final submission saved.\")\n",
    "# Save final submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Inference complete. submission.csv saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b88ce-b12b-4863-b639-c2a1c86b0bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
