{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMO8WDCktAm6Ti86DIi2+Pt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cbb435839a024635821307768f14fcd8":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_2aeb72d59bfa49b7bc9e599f297a7b2c","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[32m‚†ô\u001b[0m Waiting for authorization\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†ô</span> Waiting for authorization\n</pre>\n"},"metadata":{}}]}},"2aeb72d59bfa49b7bc9e599f297a7b2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_-SBStOa6YV","executionInfo":{"status":"ok","timestamp":1754070702687,"user_tz":-240,"elapsed":4792,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}},"outputId":"6e6ef724-6c19-4739-daf4-4d808f00aa44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.7)\n","Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.6.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: mlflow==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (8.2.1)\n","Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.2.1)\n","Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.18.0)\n","Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.4)\n","Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.1.45)\n","Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (6.0.2)\n","Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (4.25.8)\n","Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2022.7.1)\n","Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.32.3)\n","Requirement already satisfied: packaging<24 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (23.2)\n","Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (6.11.0)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.5.3)\n","Requirement already satisfied: alembic<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (1.16.4)\n","Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (6.1.3)\n","Requirement already satisfied: Flask<3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.3.3)\n","Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (1.16.1)\n","Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (1.2.4)\n","Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (2.0.42)\n","Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (11.0.0)\n","Requirement already satisfied: shap<1,>=0.40 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (0.48.0)\n","Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.8.2)\n","Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.10.0)\n","Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (20.1.0)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.2.2) (3.1.6)\n","Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n","Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.77)\n","Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n","Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\n","Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n","Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n","Requirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\n","Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n","Requirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.3)\n","Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\n","Requirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.8.0)\n","Requirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.3.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.40.0)\n","Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\n","Requirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.11)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2->mlflow==2.2.2) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic<2->mlflow==2.2.2) (4.14.1)\n","Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.3.0)\n","Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (2.11.7)\n","Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (2.10.1)\n","Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (3.3.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (0.9.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (1.17.0)\n","Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.2.2) (2.5.0)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==2.2.2) (1.8.0)\n","Requirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.2.2) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.2.2) (2.2.0)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==2.2.2) (1.9.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==2.2.2) (4.0.12)\n","Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.11/dist-packages (from gunicorn<21->mlflow==2.2.2) (75.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.2.2) (3.23.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow==2.2.2) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.2.2) (3.2.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==2.2.2) (3.4.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap<1,>=0.40->mlflow==2.2.2) (0.0.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap<1,>=0.40->mlflow==2.2.2) (0.60.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.2.2) (3.2.3)\n","Requirement already satisfied: botocore<1.41.0,>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.40.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\n","Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.13.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\n","Requirement already satisfied: graphql-core<3.2.7,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (3.2.6)\n","Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n","Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\n","Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.2.2) (5.0.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap<1,>=0.40->mlflow==2.2.2) (0.43.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.4.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.1.0)\n","Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n"]}],"source":["!pip install prophet dagshub scikit-learn pandas numpy mlflow==2.2.2"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import mlflow\n","import mlflow.pytorch\n","import dagshub\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn.preprocessing import StandardScaler\n","import warnings\n","import os\n","from datetime import datetime\n","import pickle\n","\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"McM364psa-uq","executionInfo":{"status":"ok","timestamp":1754070711884,"user_tz":-240,"elapsed":6409,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["os.environ['MLFLOW_TRACKING_USERNAME'] = 'nipkha21'\n","os.environ['MLFLOW_TRACKING_PASSWORD'] = '202fb8a4c58a90b0eb3598b1037498eb6fe9f593'\n","\n","dagshub.init(repo_owner='TomC333', repo_name='ml-walmart-recruiting', mlflow=True)\n","mlflow.set_tracking_uri('https://dagshub.com/TomC333/ml-walmart-recruiting.mlflow')\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235,"referenced_widgets":["cbb435839a024635821307768f14fcd8","2aeb72d59bfa49b7bc9e599f297a7b2c"]},"id":"OboBeYAqa-0F","executionInfo":{"status":"ok","timestamp":1754070719445,"user_tz":-240,"elapsed":5670,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}},"outputId":"bc2d750d-62cc-471a-89cc-fa52f996294f"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb435839a024635821307768f14fcd8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","Open the following link in your browser to authorize the client:\n","https://dagshub.com/login/oauth/authorize?state=9ee2b2d9-518d-4e5f-9802-3724b0d4a586&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=cdfb58b1d85c03a542d3dcdb080719a32e2dc25cea8be4a11943f0a8756c22ee\n","\n","\n"]},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Accessing as nipkha21\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as nipkha21\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Initialized MLflow to track repo \u001b[32m\"TomC333/ml-walmart-recruiting\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"TomC333/ml-walmart-recruiting\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Repository TomC333/ml-walmart-recruiting initialized!\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository TomC333/ml-walmart-recruiting initialized!\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["def load_and_merge_data():\n","    \"\"\"Load and merge all datasets\"\"\"\n","    print(\"Loading datasets...\")\n","\n","    # Load datasets\n","    features_data = pd.read_csv('features.csv')\n","    train_data = pd.read_csv('train.csv')\n","    stores = pd.read_csv('stores.csv')\n","\n","    print(f\"Features data shape: {features_data.shape}\")\n","    print(f\"Train data shape: {train_data.shape}\")\n","    print(f\"Stores data shape: {stores.shape}\")\n","\n","    # Merge datasets\n","    df = train_data.merge(features_data, on=['Store', 'Date'], how='inner').merge(stores, on=['Store'], how='inner')\n","\n","    # Handle duplicate IsHoliday columns\n","    if 'IsHoliday_y' in df.columns:\n","        df.drop(['IsHoliday_y'], axis=1, inplace=True)\n","        df.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)\n","\n","    print(f\"Merged dataset shape: {df.shape}\")\n","    print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n","\n","    return df"],"metadata":{"id":"VBX-qs4ua-20","executionInfo":{"status":"ok","timestamp":1754071052109,"user_tz":-240,"elapsed":47,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def create_comprehensive_features(df):\n","    \"\"\"Apply comprehensive feature engineering pipeline based on data exploration\"\"\"\n","    print(\"Applying comprehensive feature engineering pipeline...\")\n","\n","    # Convert Date to datetime and sort\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values(by=['Store', 'Dept', 'Date'])\n","\n","    # Remove negative sales (based on your exploration)\n","    print(f\"Negative sales records: {len(df[df['Weekly_Sales'] < 0])}\")\n","    df = df[df['Weekly_Sales'] >= 0]\n","\n","    # Create lag features for Weekly_Sales (from your data exploration)\n","    lags = [1, 2, 3, 4, 52]  # 1-4 weeks and 1 year (52 weeks)\n","    for lag in lags:\n","        df[f'Weekly_Sales_lag_{lag}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n","\n","    # Create Sales_diff1 (but we won't use it as per your note)\n","    df['Sales_diff1'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].diff()\n","\n","    # Create time features\n","    df['Year'] = df['Date'].dt.year\n","    df['Month'] = df['Date'].dt.month\n","    df['Week'] = df['Date'].dt.isocalendar().week\n","    df['Quarter'] = df['Date'].dt.quarter\n","\n","    # Create change features for external variables\n","    for col in ['Fuel_Price', 'CPI', 'Unemployment']:\n","        if col in df.columns:\n","            df[f'{col}_change'] = df.groupby(['Store', 'Dept'])[col].diff()\n","            df[f'{col}_pct_change'] = df.groupby(['Store', 'Dept'])[col].pct_change()\n","\n","    # Create lag features for external variables\n","    for lag in [1, 2, 4, 8, 12]:\n","        for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']:\n","            if col in df.columns:\n","                df[f'{col}_lag_{lag}'] = df.groupby(['Store', 'Dept'])[col].shift(lag)\n","\n","    # Create outlier features (from your data exploration)\n","    df['is_outlier'] = ((df['Weekly_Sales'] < -25108.67) | (df['Weekly_Sales'] > 847494.61)).astype(int)\n","    outlier_propensity_store = df.groupby('Store')['is_outlier'].mean()\n","    outlier_propensity_dept = df.groupby('Dept')['is_outlier'].mean()\n","    df['store_outlier_propensity'] = df['Store'].map(outlier_propensity_store)\n","    df['dept_outlier_propensity'] = df['Dept'].map(outlier_propensity_dept)\n","    df['is_outlier_lag1'] = df.groupby(['Store', 'Dept'])['is_outlier'].shift(1)\n","    df['outlier_count_last_4weeks'] = df.groupby(['Store', 'Dept'])['is_outlier'].rolling(4).sum().shift(1).reset_index(level=[0,1], drop=True)\n","    df.drop('is_outlier', axis=1, inplace=True)\n","\n","    # Create holiday flags (based on your exploration)\n","    df['Is_SuperBowl'] = np.where(df['Date'].isin(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08']), 1, 0)\n","    df['Is_LaborDay'] = np.where(df['Date'].isin(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06']), 1, 0)\n","    df['Is_Thanksgiving'] = np.where(df['Date'].isin(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29']), 1, 0)\n","    df['Is_Christmas'] = np.where(df['Date'].isin(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']), 1, 0)\n","\n","    print(\"Feature engineering completed\")\n","    print(f\"Final dataset shape: {df.shape}\")\n","\n","    return df"],"metadata":{"id":"1QIRucs4a-5Y","executionInfo":{"status":"ok","timestamp":1754072546936,"user_tz":-240,"elapsed":8,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def comprehensive_data_cleaning(df):\n","    \"\"\"Apply comprehensive data cleaning based on your exploration\"\"\"\n","    print(\"Applying comprehensive data cleaning...\")\n","\n","    # Drop markdown columns (mostly empty - from your analysis)\n","    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n","    print(\"Dropping markdown columns with >90% missing values:\")\n","    for col in markdown_cols:\n","        if col in df.columns:\n","            missing_pct = df[col].isnull().sum() / len(df) * 100\n","            print(f\"{col}: {missing_pct:.1f}% missing\")\n","            df = df.drop(columns=[col])\n","\n","    # Handle lag features properly\n","    lag_cols = [col for col in df.columns if 'lag' in col.lower()]\n","    for col in lag_cols:\n","        df[col] = df.groupby(['Store', 'Dept'])[col].ffill().bfill()\n","        df[col] = df[col].fillna(df[col].median())\n","\n","    # Handle external factor derived features\n","    external_derived_cols = [col for col in df.columns if any(x in col for x in ['change', 'pct_change'])\n","                            and any(y in col for y in ['Fuel_Price', 'CPI', 'Unemployment', 'Temperature'])]\n","\n","    for col in external_derived_cols:\n","        if col in df.columns:\n","            df[col] = df.groupby(['Store', 'Dept'])[col].ffill()\n","            df[col] = df[col].fillna(0)\n","\n","    # Handle outlier features\n","    outlier_cols = [col for col in df.columns if 'outlier' in col.lower()]\n","    for col in outlier_cols:\n","        if col in df.columns:\n","            df[col] = df[col].fillna(0)\n","\n","    # Final cleanup - handle remaining missing values\n","    remaining_missing = df.isnull().sum()\n","    remaining_missing = remaining_missing[remaining_missing > 0]\n","\n","    for col in remaining_missing.index:\n","        if df[col].dtype in ['int64', 'float64']:\n","            df[col] = df.groupby(['Store', 'Dept'])[col].ffill().bfill()\n","            df[col] = df[col].fillna(df[col].median())\n","        else:\n","            df[col] = df.groupby(['Store', 'Dept'])[col].ffill()\n","            df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown')\n","\n","    # Remove extreme outliers and normalize for stability\n","    sales_mean = df['Weekly_Sales'].mean()\n","    sales_std = df['Weekly_Sales'].std()\n","    outlier_threshold = 5 * sales_std\n","\n","    before_outlier_removal = len(df)\n","    df = df[\n","        (df['Weekly_Sales'] >= sales_mean - outlier_threshold) &\n","        (df['Weekly_Sales'] <= sales_mean + outlier_threshold)\n","    ]\n","    print(f\"Removed {before_outlier_removal - len(df)} extreme outliers\")\n","\n","    # Create normalized target for stable training\n","    df['Weekly_Sales_normalized'] = (df['Weekly_Sales'] - sales_mean) / sales_std\n","\n","    # IMPORTANT: Convert all numeric columns to float32 for consistent processing\n","    print(\"Converting numeric columns to float32...\")\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    for col in numeric_cols:\n","        if col not in ['Store', 'Dept']:  # Keep Store and Dept as int for indexing\n","            try:\n","                df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float32)\n","            except:\n","                print(f\"Warning: Could not convert {col} to float32\")\n","\n","    # Handle any NaN values created during conversion\n","    for col in numeric_cols:\n","        if col not in ['Store', 'Dept']:\n","            if df[col].isnull().any():\n","                df[col] = df[col].fillna(df[col].median())\n","\n","    # Store normalization stats for later use\n","    normalization_stats = {'mean': sales_mean, 'std': sales_std}\n","\n","    print(\"‚úÖ All missing values handled successfully!\")\n","    print(f\"Final clean dataset shape: {df.shape}\")\n","\n","    # Final verification - check data types\n","    print(\"Data types verification:\")\n","    feature_cols_to_check = ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay',\n","                            'Is_Thanksgiving', 'Is_Christmas', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","    for col in feature_cols_to_check:\n","        if col in df.columns:\n","            print(f\"  {col}: {df[col].dtype} (sample: {df[col].iloc[0]})\")\n","\n","    return df, normalization_stats"],"metadata":{"id":"dJx3tBi1a-7k","executionInfo":{"status":"ok","timestamp":1754072550200,"user_tz":-240,"elapsed":14,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["class MovingAverage(nn.Module):\n","    \"\"\"Moving average block for DLinear\"\"\"\n","    def __init__(self, kernel_size, stride):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n","\n","    def forward(self, x):\n","        # Padding\n","        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n","        x = torch.cat([front, x, end], dim=1)\n","        x = self.avg(x.permute(0, 2, 1))\n","        x = x.permute(0, 2, 1)\n","        return x\n","\n","class SeriesDecomposition(nn.Module):\n","    \"\"\"Series decomposition block for DLinear\"\"\"\n","    def __init__(self, kernel_size):\n","        super().__init__()\n","        self.moving_avg = MovingAverage(kernel_size, stride=1)\n","\n","    def forward(self, x):\n","        moving_mean = self.moving_avg(x)\n","        residual = x - moving_mean\n","        return residual, moving_mean\n","\n","class DLinear(nn.Module):\n","    \"\"\"DLinear: Decomposition Linear for time series forecasting\"\"\"\n","    def __init__(self, seq_len, pred_len, enc_in=1, individual=False, kernel_size=25):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.pred_len = pred_len\n","        self.individual = individual\n","        self.channels = enc_in\n","\n","        print(f\"Creating DLinear with {enc_in} input channels\")\n","\n","        # Decomposition\n","        self.decomposition = SeriesDecomposition(kernel_size)\n","\n","        if self.individual:\n","            # Individual linear layers for each channel\n","            self.Linear_Seasonal = nn.ModuleList([\n","                nn.Linear(self.seq_len, self.pred_len) for _ in range(self.channels)\n","            ])\n","            self.Linear_Trend = nn.ModuleList([\n","                nn.Linear(self.seq_len, self.pred_len) for _ in range(self.channels)\n","            ])\n","        else:\n","            # Shared linear layers - but we need to handle multiple channels properly\n","            # Each channel gets processed through the same linear layers\n","            self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n","            self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n","\n","            # If we have multiple input features, we need to project them to 1 output channel\n","            if self.channels > 1:\n","                self.feature_projection = nn.Linear(self.channels, 1)\n","                print(f\"Added feature projection layer: {self.channels} -> 1\")\n","            else:\n","                self.feature_projection = None\n","\n","    def forward(self, x):\n","        # x: [Batch, Input length, Channel]\n","        batch_size, seq_len, n_channels = x.shape\n","\n","        target_channel = x[:, :, 0:1]  # [Batch, seq_len, 1]\n","        seasonal_init, trend_init = self.decomposition(target_channel)\n","\n","        # If we have additional features, use them to modulate the linear layers\n","        if self.channels > 1 and self.feature_projection is not None:\n","            # Project all features to get feature influence\n","            feature_weights = torch.sigmoid(self.feature_projection(x.mean(dim=1)))  # [Batch, 1]\n","            feature_weights = feature_weights.unsqueeze(1)  # [Batch, 1, 1]\n","        else:\n","            feature_weights = torch.ones(x.size(0), 1, 1, device=x.device)\n","        if self.training and torch.rand(1).item() < 0.01:  # Log 1% of training batches\n","          print(f\"Feature weights stats: mean={feature_weights.mean():.4f}, std={feature_weights.std():.4f}\")\n","          print(f\"Feature weights range: {feature_weights.min():.4f} to {feature_weights.max():.4f}\")\n","\n","        # Permute for linear layers: [Batch, seq_len, 1] -> [Batch, 1, seq_len]\n","        seasonal_init = seasonal_init.permute(0, 2, 1)\n","        trend_init = trend_init.permute(0, 2, 1)\n","\n","        if self.individual:\n","          seasonal_output = torch.zeros([batch_size, 1, self.pred_len], dtype=x.dtype, device=x.device)\n","          trend_output = torch.zeros([batch_size, 1, self.pred_len], dtype=x.dtype, device=x.device)\n","\n","          seasonal_output[:, 0, :] = self.Linear_Seasonal[0](seasonal_init[:, 0, :]) * feature_weights.squeeze(-1)\n","          trend_output[:, 0, :] = self.Linear_Trend[0](trend_init[:, 0, :]) * feature_weights.squeeze(-1)\n","        else:\n","            # Apply linear layers: [Batch, 1, seq_len] -> [Batch, 1, pred_len]\n","            seasonal_output = self.Linear_Seasonal(seasonal_init) * feature_weights\n","            trend_output = self.Linear_Trend(trend_init) * feature_weights\n","\n","        # Combine seasonal and trend\n","        output = seasonal_output + trend_output\n","\n","        # Return: [Batch, pred_len, 1]\n","        return output.permute(0, 2, 1)\n","\n","    def count_parameters(self):\n","        \"\"\"Count total parameters in the model\"\"\"\n","        total = sum(p.numel() for p in self.parameters() if p.requires_grad)\n","        print(f\"DLinear parameters breakdown:\")\n","        print(f\"  Seasonal Linear: {sum(p.numel() for p in self.Linear_Seasonal.parameters())}\")\n","        print(f\"  Trend Linear: {sum(p.numel() for p in self.Linear_Trend.parameters())}\")\n","        if hasattr(self, 'feature_projection') and self.feature_projection is not None:\n","            print(f\"  Feature Projection: {sum(p.numel() for p in self.feature_projection.parameters())}\")\n","        print(f\"  Total: {total}\")\n","        return total"],"metadata":{"id":"j8hu6hsKtH3i","executionInfo":{"status":"ok","timestamp":1754073407299,"user_tz":-240,"elapsed":5,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["class WalmartDLinearDataset(Dataset):\n","    def __init__(self, data, store_dept_combinations, seq_len, pred_len,\n","                 feature_cols=None, target_col='Weekly_Sales_normalized'):\n","        self.data = data\n","        self.combinations = store_dept_combinations\n","        self.seq_len = seq_len\n","        self.pred_len = pred_len\n","        self.feature_cols = feature_cols or []\n","        self.target_col = target_col\n","        self.samples = []\n","\n","        print(f\"Creating dataset with {len(self.feature_cols)} features: {self.feature_cols}\")\n","\n","        # Create samples for each store-dept combination\n","        for _, row in store_dept_combinations.iterrows():\n","            store, dept = row['Store'], row['Dept']\n","            store_dept_data = data[(data['Store'] == store) & (data['Dept'] == dept)].sort_values('Date')\n","\n","            # Remove any remaining NaN rows for this combination\n","            store_dept_data = store_dept_data.dropna(subset=[target_col])\n","\n","            if len(store_dept_data) >= seq_len + pred_len:\n","                # Create sliding windows\n","                for i in range(len(store_dept_data) - seq_len - pred_len + 1):\n","                    sample_data = store_dept_data.iloc[i:i + seq_len + pred_len]\n","\n","                    # Extract target sequence\n","                    target_seq = sample_data[target_col].values.astype(np.float32)\n","                    input_seq = target_seq[:seq_len]\n","                    output_seq = target_seq[seq_len:]\n","\n","                    # Check for NaN in target sequences\n","                    if np.any(pd.isna(input_seq)) or np.any(pd.isna(output_seq)):\n","                        continue\n","\n","                    # Extract features if specified\n","                    if self.feature_cols:\n","                      # Use normalized versions of features\n","                      normalized_feature_cols = [f\"{col}_normalized\" if f\"{col}_normalized\" in sample_data.columns else col\n","                                                for col in self.feature_cols]\n","                      feature_data = sample_data[normalized_feature_cols].iloc[:seq_len]\n","\n","                        # Check for NaN in features using pandas\n","                      if feature_data.isnull().any().any():\n","                          continue\n","\n","                      # Convert features to float32 and handle any remaining issues\n","                      try:\n","                          feature_seq = feature_data.values.astype(np.float32)\n","                      except (ValueError, TypeError) as e:\n","                          print(f\"Feature conversion error for store {store}, dept {dept}: {e}\")\n","                          continue\n","\n","                      # Combine target and features: [seq_len, 1 + n_features]\n","                      combined_input = np.column_stack([input_seq.reshape(-1, 1), feature_seq])\n","                    else:\n","                        combined_input = input_seq.reshape(-1, 1)\n","\n","                    # Final check for any problematic values using pandas which handles mixed types better\n","                    combined_df = pd.DataFrame(combined_input)\n","                    output_df = pd.DataFrame(output_seq)\n","\n","                    if combined_df.isnull().any().any() or output_df.isnull().any().any():\n","                        continue\n","\n","                    # Check for infinite values\n","                    if np.any(np.isinf(combined_input)) or np.any(np.isinf(output_seq)):\n","                        continue\n","\n","                    # Ensure everything is float32\n","                    combined_input = combined_input.astype(np.float32)\n","                    output_seq = output_seq.astype(np.float32)\n","\n","                    self.samples.append({\n","                        'input': combined_input,\n","                        'target': output_seq,\n","                        'store': store,\n","                        'dept': dept\n","                    })\n","\n","        print(f\"Created {len(self.samples)} valid samples\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample = self.samples[idx]\n","\n","        input_tensor = torch.FloatTensor(sample['input'])  # [seq_len, n_features]\n","        target_tensor = torch.FloatTensor(sample['target'])  # [pred_len]\n","\n","        return input_tensor, target_tensor, sample['store'], sample['dept']"],"metadata":{"id":"XlcYD5Iva--J","executionInfo":{"status":"ok","timestamp":1754073232895,"user_tz":-240,"elapsed":11,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["def train_dlinear_model(train_loader, val_loader, model, epochs=100, lr=1e-3, patience=10, device='cuda'):\n","    \"\"\"Train DLinear model with robust error handling\"\"\"\n","    model = model.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n","    criterion = nn.MSELoss()\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    print(f\"üöÄ Training DLinear on {device}\")\n","\n","    for epoch in range(epochs):\n","        # Training\n","        model.train()\n","        train_loss = 0.0\n","        valid_batches = 0\n","\n","        for batch_idx, (input_seq, target_seq, _, _) in enumerate(train_loader):\n","            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n","\n","            # Check for NaN in batch\n","            if torch.isnan(input_seq).any() or torch.isnan(target_seq).any():\n","                continue\n","\n","            optimizer.zero_grad()\n","\n","            pred_seq = model(input_seq)  # [batch, pred_len, n_channels]\n","            pred_seq = pred_seq[:, :, 0]  # Only use first channel (target predictions)\n","            loss = criterion(pred_seq, target_seq)\n","\n","            # Check for NaN loss\n","            if torch.isnan(loss):\n","                continue\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            valid_batches += 1\n","\n","        if valid_batches == 0:\n","            print(\"No valid batches in training, stopping...\")\n","            break\n","\n","        train_loss /= valid_batches\n","        train_losses.append(train_loss)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        valid_val_batches = 0\n","\n","        with torch.no_grad():\n","            for input_seq, target_seq, _, _ in val_loader:\n","                input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n","\n","                if torch.isnan(input_seq).any() or torch.isnan(target_seq).any():\n","                    continue\n","\n","                pred_seq = model(input_seq)[:, :, 0]\n","                loss = criterion(pred_seq, target_seq)\n","\n","                if not torch.isnan(loss):\n","                    val_loss += loss.item()\n","                    valid_val_batches += 1\n","\n","        if valid_val_batches == 0:\n","            val_loss = train_loss\n","        else:\n","            val_loss /= valid_val_batches\n","\n","        val_losses.append(val_loss)\n","        scheduler.step(val_loss)\n","\n","        # Early stopping\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            patience_counter = 0\n","            best_model_state = model.state_dict().copy()\n","        else:\n","            patience_counter += 1\n","\n","        if patience_counter >= patience:\n","            print(f\"Early stopping at epoch {epoch+1}\")\n","            break\n","\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","    # Load best model\n","    model.load_state_dict(best_model_state)\n","    return model, train_losses, val_losses, best_val_loss"],"metadata":{"id":"xSuVqwpPa_BA","executionInfo":{"status":"ok","timestamp":1754073235762,"user_tz":-240,"elapsed":17,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["def evaluate_dlinear_model(model, test_loader, device='cuda'):\n","    \"\"\"Evaluate DLinear model\"\"\"\n","    model = model.to(device)\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","\n","    with torch.no_grad():\n","        for input_seq, target_seq, _, _ in test_loader:\n","            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n","\n","            if torch.isnan(input_seq).any() or torch.isnan(target_seq).any():\n","                continue\n","\n","            pred_seq = model(input_seq)[:, :, 0]\n","\n","            if torch.isnan(pred_seq).any():\n","                continue\n","\n","            predictions.extend(pred_seq.cpu().numpy())\n","            actuals.extend(target_seq.cpu().numpy())\n","\n","    if len(predictions) == 0:\n","        return float('inf'), float('inf'), float('inf'), [], []\n","\n","    predictions = np.array(predictions)\n","    actuals = np.array(actuals)\n","\n","    # Calculate metrics\n","    mae = mean_absolute_error(actuals.flatten(), predictions.flatten())\n","    rmse = np.sqrt(mean_squared_error(actuals.flatten(), predictions.flatten()))\n","    mask = np.abs(actuals.flatten()) > 0.01  # Only calculate MAPE for non-zero actuals\n","    if mask.sum() > 0:\n","        mape = np.mean(np.abs((actuals.flatten()[mask] - predictions.flatten()[mask]) / actuals.flatten()[mask])) * 100\n","    else:\n","        mape = float('inf')\n","\n","    return mae, rmse, mape, predictions, actuals"],"metadata":{"id":"Nt5jjx6Ma_Dg","executionInfo":{"status":"ok","timestamp":1754073238443,"user_tz":-240,"elapsed":5,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["mlflow.set_experiment(\"DLinear_Training\")\n","\n","# Load and prepare data\n","with mlflow.start_run(run_name=\"DLinear_Data_Preprocessing\"):\n","    print(\"=== DLinear Data Preprocessing ===\")\n","\n","    # Load data\n","    df = load_and_merge_data()\n","    mlflow.log_param(\"initial_data_shape\", df.shape)\n","    mlflow.log_param(\"date_range\", f\"{df['Date'].min()} to {df['Date'].max()}\")\n","\n","    # Feature engineering\n","    df = create_comprehensive_features(df)\n","    mlflow.log_param(\"post_feature_engineering_shape\", df.shape)\n","\n","    # Data cleaning\n","    df, norm_stats = comprehensive_data_cleaning(df)\n","    mlflow.log_param(\"final_clean_data_shape\", df.shape)\n","    mlflow.log_param(\"normalization_mean\", norm_stats['mean'])\n","    mlflow.log_param(\"normalization_std\", norm_stats['std'])\n","\n","    # Save normalization stats for inference\n","    with open('dlinear_normalization_stats.pkl', 'wb') as f:\n","        pickle.dump(norm_stats, f)\n","    mlflow.log_artifact('dlinear_normalization_stats.pkl')\n","\n","    print(\"‚úÖ Data preprocessing completed\")\n","\n","# Configuration\n","SEQ_LEN = 20\n","PRED_LEN = 10\n","BATCH_SIZE = 128 if device.type == 'cuda' else 64\n","MIN_DATA_POINTS = 30\n","TRAIN_RATIO = 0.7\n","VAL_RATIO = 0.15\n","\n","# Define iterative phases (following your lecturer's guidance)\n","phases = {\n","    \"Phase1_Basic\": {\n","        \"features\": [],  # Only time series data\n","        \"description\": \"Basic DLinear with only Weekly_Sales time series\"\n","    },\n","    \"Phase2_Time\": {\n","        \"features\": ['Month', 'Week', 'Quarter'],\n","        \"description\": \"DLinear with time features\"\n","    },\n","    \"Phase3_Holidays\": {\n","        \"features\": ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas'],\n","        \"description\": \"DLinear with time and holiday features\"\n","    },\n","    \"Phase4_External\": {\n","        \"features\": ['Month', 'Week', 'Quarter', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment'],\n","        \"description\": \"DLinear with time, holiday, and external features\"\n","    },\n","    \"Phase5_Lags\": {\n","        \"features\": ['Month', 'Week', 'Quarter', 'IsHoliday', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4'],\n","        \"description\": \"DLinear with time and lag features (no Sales_diff1 as per your note)\"\n","    },\n","    \"Phase6_Full\": {\n","        \"features\": ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas',\n","                    'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n","                    'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4',\n","                    'store_outlier_propensity', 'dept_outlier_propensity'],\n","        \"description\": \"DLinear with comprehensive feature set (excluding Sales_diff1)\"\n","    }\n","}\n","\n","# Prepare train/val/test splits\n","store_dept_combinations = df.groupby(['Store', 'Dept']).size().reset_index(name='count')\n","valid_combinations = store_dept_combinations[store_dept_combinations['count'] >= MIN_DATA_POINTS][['Store', 'Dept']]\n","\n","# Use subset for experimentation\n","valid_combinations = valid_combinations.head(50)\n","\n","n_train = int(len(valid_combinations) * TRAIN_RATIO)\n","n_val = int(len(valid_combinations) * VAL_RATIO)\n","\n","train_combinations = valid_combinations[:n_train]\n","val_combinations = valid_combinations[n_train:n_train+n_val]\n","test_combinations = valid_combinations[n_train+n_val:]\n","\n","print(f\"Train combinations: {len(train_combinations)}\")\n","print(f\"Validation combinations: {len(val_combinations)}\")\n","print(f\"Test combinations: {len(test_combinations)}\")\n","\n","# Store results for comparison\n","all_results = {}\n","best_model = None\n","best_mae = float('inf')\n","best_phase = None\n","\n","# Run iterative phases\n","for phase_name, phase_config in phases.items():\n","    print(f\"\\n{'='*60}\")\n","    print(f\"PHASE: {phase_name}\")\n","    print(f\"Description: {phase_config['description']}\")\n","    print(f\"Features: {len(phase_config['features'])} features\")\n","    print(f\"{'='*60}\")\n","\n","    with mlflow.start_run(run_name=f\"DLinear_{phase_name}\"):\n","\n","        # Log phase parameters\n","        mlflow.log_param(\"phase_name\", phase_name)\n","        mlflow.log_param(\"features\", phase_config['features'])\n","        mlflow.log_param(\"n_features\", len(phase_config['features']))\n","        mlflow.log_param(\"seq_len\", SEQ_LEN)\n","        mlflow.log_param(\"pred_len\", PRED_LEN)\n","        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n","        mlflow.log_param(\"device\", str(device))\n","\n","        try:\n","            # Create datasets\n","            train_dataset = WalmartDLinearDataset(\n","                df, train_combinations, SEQ_LEN, PRED_LEN,\n","                feature_cols=phase_config['features']\n","            )\n","            if len(train_dataset) > 0 and len(phase_config['features']) > 0:\n","              sample_input, _, _, _ = train_dataset[0]\n","              print(f\"Feature scaling check:\")\n","              print(f\"  Input shape: {sample_input.shape}\")\n","              print(f\"  Target (col 0) range: {sample_input[:, 0].min():.3f} to {sample_input[:, 0].max():.3f}\")\n","              if sample_input.shape[1] > 1:\n","                  print(f\"  Features (col 1+) range: {sample_input[:, 1:].min():.3f} to {sample_input[:, 1:].max():.3f}\")\n","            if len(train_dataset) > 5 and len(phase_config['features']) > 0:\n","              print(\"Checking feature variation across samples:\")\n","              samples = [train_dataset[i][0] for i in range(5)]  # Get 5 samples\n","              for i, sample in enumerate(samples):\n","                  if sample.shape[1] > 1:\n","                      features = sample[:, 1:].mean(dim=0)  # Average features over time\n","                      print(f\"  Sample {i} features: {features.numpy()}\")\n","            val_dataset = WalmartDLinearDataset(\n","                df, val_combinations, SEQ_LEN, PRED_LEN,\n","                feature_cols=phase_config['features']\n","            )\n","            test_dataset = WalmartDLinearDataset(\n","                df, test_combinations, SEQ_LEN, PRED_LEN,\n","                feature_cols=phase_config['features']\n","            )\n","\n","            print(f\"Train samples: {len(train_dataset)}\")\n","            print(f\"Val samples: {len(val_dataset)}\")\n","            print(f\"Test samples: {len(test_dataset)}\")\n","\n","            if len(train_dataset) == 0:\n","                print(\"No training samples available, skipping phase\")\n","                mlflow.log_param(\"error\", \"No training samples\")\n","                continue\n","\n","            # Create data loaders\n","            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n","            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n","\n","            # Create model - FIXED DIMENSION CALCULATION\n","            n_features = len(phase_config['features']) + 1  # +1 for Weekly_Sales\n","\n","            # Debug: Print actual feature dimensions\n","            print(f\"Expected features: {phase_config['features']}\")\n","            print(f\"Expected n_features: {n_features}\")\n","\n","            # Verify by checking actual dataset sample\n","            if len(train_dataset) > 0:\n","                sample_input, _, _, _ = train_dataset[0]\n","                actual_features = sample_input.shape[1]\n","                print(f\"Actual input features from dataset: {actual_features}\")\n","\n","                # Use the actual feature count from dataset\n","                n_features = actual_features\n","            else:\n","                print(\"No training samples to verify feature count\")\n","\n","            print(f\"Creating model with {n_features} input features...\")\n","\n","            model = DLinear(\n","                seq_len=SEQ_LEN,\n","                pred_len=PRED_LEN,\n","                enc_in=n_features,\n","                individual=False,\n","                kernel_size=25\n","            )\n","\n","            total_params = sum(p.numel() for p in model.parameters())\n","            print(f\"Model created with {total_params:,} parameters\")\n","\n","            # Verify model input/output shapes\n","            with torch.no_grad():\n","                if len(train_dataset) > 0:\n","                    sample_input, sample_target, _, _ = train_dataset[0]\n","                    sample_input = sample_input.unsqueeze(0)  # Add batch dim\n","                    print(f\"Sample input shape: {sample_input.shape}\")\n","                    print(f\"Sample target shape: {sample_target.shape}\")\n","\n","                    # Test forward pass\n","                    test_output = model(sample_input)\n","                    print(f\"Model output shape: {test_output.shape}\")\n","                    print(f\"Expected output shape: [1, {PRED_LEN}, 1]\")\n","\n","            mlflow.log_param(\"total_parameters\", total_params)\n","            mlflow.log_param(\"actual_n_features\", n_features)\n","\n","            # Train model\n","            print(\"Training DLinear model...\")\n","            print(f\"Training with features: {phase_config['features']}\")\n","            print(f\"Model expects {n_features} features\")\n","\n","            model, train_losses, val_losses, best_val_loss = train_dlinear_model(\n","                train_loader, val_loader, model, epochs=50, lr=1e-3, patience=8, device=device\n","            )\n","\n","            # Print parameter breakdown\n","            model.count_parameters()\n","\n","            # Evaluate model\n","            print(\"Evaluating model...\")\n","            mae, rmse, mape, predictions, actuals = evaluate_dlinear_model(model, test_loader, device)\n","\n","            # Additional debugging: check if predictions vary\n","            if len(predictions) > 0:\n","                pred_flat = predictions.flatten()\n","                print(f\"Prediction statistics:\")\n","                print(f\"  Mean: {pred_flat.mean():.4f}\")\n","                print(f\"  Std: {pred_flat.std():.4f}\")\n","                print(f\"  Min: {pred_flat.min():.4f}\")\n","                print(f\"  Max: {pred_flat.max():.4f}\")\n","                print(f\"  Unique values: {len(np.unique(pred_flat))}\")\n","\n","                # Check if model is just predicting the same value\n","                if pred_flat.std() < 1e-6:\n","                    print(\"‚ö†Ô∏è WARNING: Model is predicting nearly constant values!\")\n","\n","                # Check actual vs predicted sample\n","                if len(actuals) > 0:\n","                    actual_flat = actuals.flatten()\n","                    print(f\"Actual statistics:\")\n","                    print(f\"  Mean: {actual_flat.mean():.4f}\")\n","                    print(f\"  Std: {actual_flat.std():.4f}\")\n","                    print(f\"  Min: {actual_flat.min():.4f}\")\n","                    print(f\"  Max: {actual_flat.max():.4f}\")\n","\n","            # Log metrics\n","            mlflow.log_metric(\"best_val_loss\", best_val_loss)\n","            mlflow.log_metric(\"test_mae\", mae)\n","            mlflow.log_metric(\"test_rmse\", rmse)\n","            mlflow.log_metric(\"test_mape\", mape)\n","            mlflow.log_metric(\"n_train_samples\", len(train_dataset))\n","            mlflow.log_metric(\"n_val_samples\", len(val_dataset))\n","            mlflow.log_metric(\"n_test_samples\", len(test_dataset))\n","\n","            # Log prediction statistics\n","            if len(predictions) > 0:\n","                pred_flat = predictions.flatten()\n","                mlflow.log_metric(\"pred_mean\", pred_flat.mean())\n","                mlflow.log_metric(\"pred_std\", pred_flat.std())\n","                mlflow.log_metric(\"pred_range\", pred_flat.max() - pred_flat.min())\n","                mlflow.log_metric(\"n_unique_predictions\", len(np.unique(pred_flat)))\n","\n","            # Store results for comparison\n","            phase_results = {\n","                'Phase': phase_name,\n","                'Description': phase_config['description'],\n","                'N_Features': len(phase_config['features']),\n","                'MAE': mae,\n","                'RMSE': rmse,\n","                'MAPE': mape,\n","                'Val_Loss': best_val_loss,\n","                'Train_Samples': len(train_dataset),\n","                'Test_Samples': len(test_dataset),\n","                'Parameters': total_params\n","            }\n","            all_results[phase_name] = phase_results\n","\n","            # Track best model\n","            if mae < best_mae:\n","                best_mae = mae\n","                best_model = model\n","                best_phase = phase_name\n","\n","                # Log best model to MLflow\n","                mlflow.pytorch.log_model(model, f\"best_model_{phase_name}\")\n","\n","                # Also save model state dict\n","                torch.save({\n","                    'model_state_dict': model.state_dict(),\n","                    'phase_name': phase_name,\n","                    'features': phase_config['features'],\n","                    'seq_len': SEQ_LEN,\n","                    'pred_len': PRED_LEN,\n","                    'n_features': n_features,\n","                    'mae': mae,\n","                    'rmse': rmse,\n","                    'mape': mape\n","                }, f'best_dlinear_model_{phase_name}.pth')\n","                mlflow.log_artifact(f'best_dlinear_model_{phase_name}.pth')\n","\n","            print(f\"\\nPHASE {phase_name} RESULTS:\")\n","            print(f\"Test MAE: {mae:.2f}\")\n","            print(f\"Test RMSE: {rmse:.2f}\")\n","            print(f\"Test MAPE: {mape:.2f}%\")\n","            print(f\"Best Val Loss: {best_val_loss:.4f}\")\n","            print(f\"Model Parameters: {total_params:,}\")\n","\n","            # Log training history\n","            for i, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses)):\n","                mlflow.log_metric(\"epoch_train_loss\", train_loss, step=i)\n","                mlflow.log_metric(\"epoch_val_loss\", val_loss, step=i)\n","\n","        except Exception as e:\n","            print(f\"Error in phase {phase_name}: {e}\")\n","            mlflow.log_param(\"error\", str(e))\n","            import traceback\n","            print(traceback.format_exc())\n","            continue\n","\n","# Log final best model to Model Registry\n","if best_model is not None:\n","    print(f\"\\nüèÜ BEST MODEL: {best_phase} with MAE: {best_mae:.2f}\")\n","\n","    with mlflow.start_run(run_name=\"DLinear_Best_Model_Registry\"):\n","        # Log the best model\n","        mlflow.log_param(\"best_phase\", best_phase)\n","        mlflow.log_param(\"best_mae\", best_mae)\n","        mlflow.log_param(\"features\", all_results[best_phase]['N_Features'])\n","\n","        # Log model to MLflow\n","        model_info = mlflow.pytorch.log_model(\n","            best_model,\n","            \"best_dlinear_model\",\n","            registered_model_name=\"WalmartSales_DLinear_Best\"\n","        )\n","\n","        # Save complete model info for inference\n","        best_config = phases[best_phase]\n","        model_metadata = {\n","            'phase_name': best_phase,\n","            'features': best_config['features'],\n","            'seq_len': SEQ_LEN,\n","            'pred_len': PRED_LEN,\n","            'n_features': len(best_config['features']) + 1,\n","            'mae': best_mae,\n","            'model_uri': model_info.model_uri,\n","            'normalization_stats': norm_stats\n","        }\n","\n","        with open('best_dlinear_metadata.pkl', 'wb') as f:\n","            pickle.dump(model_metadata, f)\n","        mlflow.log_artifact('best_dlinear_metadata.pkl')\n","\n","        print(f\"‚úÖ Best model registered in MLflow Model Registry\")\n","        print(f\"Model URI: {model_info.model_uri}\")\n","\n","# Create comparison summary\n","results_df = pd.DataFrame(list(all_results.values()))\n","results_df = results_df.sort_values('MAE')\n","\n","print(f\"\\n{'='*80}\")\n","print(\"DLINEAR EXPERIMENT RESULTS SUMMARY\")\n","print(f\"{'='*80}\")\n","print(results_df.to_string(index=False))\n","\n","# Save results summary\n","results_df.to_csv('dlinear_experiment_results.csv', index=False)\n","\n","with mlflow.start_run(run_name=\"DLinear_Experiment_Summary\"):\n","    mlflow.log_artifact('dlinear_experiment_results.csv')\n","    mlflow.log_param(\"total_phases\", len(phases))\n","    mlflow.log_param(\"best_phase\", best_phase)\n","    mlflow.log_metric(\"best_mae\", best_mae)\n","\n","    # Log summary table as text\n","    with open('experiment_summary.txt', 'w') as f:\n","        f.write(\"DLinear Experiment Results Summary\\n\")\n","        f.write(\"=\"*50 + \"\\n\\n\")\n","        f.write(results_df.to_string(index=False))\n","        f.write(f\"\\n\\nBest Model: {best_phase}\")\n","        f.write(f\"\\nBest MAE: {best_mae:.2f}\")\n","\n","    mlflow.log_artifact('experiment_summary.txt')\n","\n","print(f\"\\n‚úÖ All DLinear experiments completed!\")\n","print(f\"üèÜ Best model: {best_phase} with MAE: {best_mae:.2f}\")\n","return best_model, best_phase, all_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9LCQR-Ora_GH","executionInfo":{"status":"error","timestamp":1754073981745,"user_tz":-240,"elapsed":569601,"user":{"displayName":"TinySalmon 90","userId":"18236352364698144937"}},"outputId":"cede389d-0264-444a-ca21-b08df5377f6d"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["=== DLinear Data Preprocessing ===\n","Loading datasets...\n","Features data shape: (8190, 12)\n","Train data shape: (421570, 5)\n","Stores data shape: (45, 3)\n","Merged dataset shape: (421570, 16)\n","Date range: 2010-02-05 to 2012-10-26\n","Applying comprehensive feature engineering pipeline...\n","Negative sales records: 1285\n","Feature engineering completed\n","Final dataset shape: (420285, 60)\n","Applying comprehensive data cleaning...\n","Dropping markdown columns with >90% missing values:\n","MarkDown1: 64.3% missing\n","MarkDown2: 73.6% missing\n","MarkDown3: 67.5% missing\n","MarkDown4: 68.0% missing\n","MarkDown5: 64.1% missing\n","Removed 2151 extreme outliers\n","Converting numeric columns to float32...\n","‚úÖ All missing values handled successfully!\n","Final clean dataset shape: (418134, 56)\n","Data types verification:\n","  Month: float32 (sample: 2.0)\n","  Week: float32 (sample: 5.0)\n","  Quarter: float32 (sample: 1.0)\n","  IsHoliday: bool (sample: False)\n","  Is_SuperBowl: float32 (sample: 0.0)\n","  Is_LaborDay: float32 (sample: 0.0)\n","  Is_Thanksgiving: float32 (sample: 0.0)\n","  Is_Christmas: float32 (sample: 0.0)\n","  Temperature: float32 (sample: 42.310001373291016)\n","  Fuel_Price: float32 (sample: 2.572000026702881)\n","  CPI: float32 (sample: 211.0963592529297)\n","  Unemployment: float32 (sample: 8.105999946594238)\n","‚úÖ Data preprocessing completed\n","Train combinations: 35\n","Validation combinations: 7\n","Test combinations: 8\n","\n","============================================================\n","PHASE: Phase1_Basic\n","Description: Basic DLinear with only Weekly_Sales time series\n","Features: 0 features\n","============================================================\n","Creating dataset with 0 features: []\n","Created 3957 valid samples\n","Creating dataset with 0 features: []\n","Created 726 valid samples\n","Creating dataset with 0 features: []\n","Created 770 valid samples\n","Train samples: 3957\n","Val samples: 726\n","Test samples: 770\n","Expected features: []\n","Expected n_features: 1\n","Actual input features from dataset: 1\n","Creating model with 1 input features...\n","Creating DLinear with 1 input channels\n","Model created with 420 parameters\n","Sample input shape: torch.Size([1, 20, 1])\n","Sample target shape: torch.Size([10])\n","Model output shape: torch.Size([1, 10, 1])\n","Expected output shape: [1, 10, 1]\n","Training DLinear model...\n","Training with features: []\n","Model expects 1 features\n","üöÄ Training DLinear on cuda\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Epoch 10/50, Train Loss: 0.0831, Val Loss: 0.0597\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Epoch 20/50, Train Loss: 0.0814, Val Loss: 0.0591\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Epoch 30/50, Train Loss: 0.0794, Val Loss: 0.0584\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Early stopping at epoch 40\n","DLinear parameters breakdown:\n","  Seasonal Linear: 210\n","  Trend Linear: 210\n","  Total: 420\n","Evaluating model...\n","Prediction statistics:\n","  Mean: -0.3563\n","  Std: 0.3042\n","  Min: -0.6953\n","  Max: 1.2346\n","  Unique values: 7673\n","Actual statistics:\n","  Mean: -0.3734\n","  Std: 0.3441\n","  Min: -0.7052\n","  Max: 1.7071\n"]},{"output_type":"stream","name":"stderr","text":["2025/08/01 18:37:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n","2025/08/01 18:37:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"]},{"output_type":"stream","name":"stdout","text":["\n","PHASE Phase1_Basic RESULTS:\n","Test MAE: 0.09\n","Test RMSE: 0.17\n","Test MAPE: 48.45%\n","Best Val Loss: 0.0582\n","Model Parameters: 420\n","\n","============================================================\n","PHASE: Phase2_Time\n","Description: DLinear with time features\n","Features: 3 features\n","============================================================\n","Creating dataset with 3 features: ['Month', 'Week', 'Quarter']\n","Created 3957 valid samples\n","Feature scaling check:\n","  Input shape: torch.Size([20, 4])\n","  Target (col 0) range: -0.055 to 1.814\n","  Features (col 1+) range: 1.000 to 24.000\n","Checking feature variation across samples:\n","  Sample 0 features: [ 3.9 14.5  1.6]\n","  Sample 1 features: [ 4.1  15.5   1.65]\n","  Sample 2 features: [ 4.35 16.5   1.75]\n","  Sample 3 features: [ 4.6  17.5   1.85]\n","  Sample 4 features: [ 4.85 18.5   1.95]\n","Creating dataset with 3 features: ['Month', 'Week', 'Quarter']\n","Created 726 valid samples\n","Creating dataset with 3 features: ['Month', 'Week', 'Quarter']\n","Created 770 valid samples\n","Train samples: 3957\n","Val samples: 726\n","Test samples: 770\n","Expected features: ['Month', 'Week', 'Quarter']\n","Expected n_features: 4\n","Actual input features from dataset: 4\n","Creating model with 4 input features...\n","Creating DLinear with 4 input channels\n","Added feature projection layer: 4 -> 1\n","Model created with 425 parameters\n","Sample input shape: torch.Size([1, 20, 4])\n","Sample target shape: torch.Size([10])\n","Model output shape: torch.Size([1, 10, 1])\n","Expected output shape: [1, 10, 1]\n","Training DLinear model...\n","Training with features: ['Month', 'Week', 'Quarter']\n","Model expects 4 features\n","üöÄ Training DLinear on cuda\n","Feature weights stats: mean=0.2297, std=0.1061\n","Feature weights range: 0.0625 to 0.4919\n","Feature weights stats: mean=0.4510, std=0.0840\n","Feature weights range: 0.2559 to 0.6090\n","Feature weights stats: mean=0.7093, std=0.0306\n","Feature weights range: 0.6322 to 0.7595\n","Epoch 10/50, Train Loss: 0.0839, Val Loss: 0.0889\n","Feature weights stats: mean=0.6721, std=0.0235\n","Feature weights range: 0.6114 to 0.7034\n","Epoch 20/50, Train Loss: 0.0806, Val Loss: 0.0837\n","Feature weights stats: mean=0.6570, std=0.0115\n","Feature weights range: 0.6249 to 0.6711\n","Feature weights stats: mean=0.6568, std=0.0119\n","Feature weights range: 0.6276 to 0.6730\n","Feature weights stats: mean=0.6558, std=0.0113\n","Feature weights range: 0.6212 to 0.6702\n","Feature weights stats: mean=0.6613, std=0.0090\n","Feature weights range: 0.6365 to 0.6734\n","Epoch 30/50, Train Loss: 0.0801, Val Loss: 0.0832\n","Feature weights stats: mean=0.6540, std=0.0089\n","Feature weights range: 0.6307 to 0.6670\n","Feature weights stats: mean=0.6456, std=0.0100\n","Feature weights range: 0.6199 to 0.6626\n","Feature weights stats: mean=0.6495, std=0.0091\n","Feature weights range: 0.6285 to 0.6654\n","Feature weights stats: mean=0.6402, std=0.0118\n","Feature weights range: 0.6116 to 0.6619\n","Feature weights stats: mean=0.6453, std=0.0101\n","Feature weights range: 0.6192 to 0.6621\n","Feature weights stats: mean=0.6347, std=0.0124\n","Feature weights range: 0.5985 to 0.6593\n","Early stopping at epoch 36\n","DLinear parameters breakdown:\n","  Seasonal Linear: 210\n","  Trend Linear: 210\n","  Feature Projection: 5\n","  Total: 425\n","Evaluating model...\n","Prediction statistics:\n","  Mean: -0.3581\n","  Std: 0.3094\n","  Min: -0.7114\n","  Max: 1.1841\n","  Unique values: 7677\n","Actual statistics:\n","  Mean: -0.3734\n","  Std: 0.3441\n","  Min: -0.7052\n","  Max: 1.7071\n"]},{"output_type":"stream","name":"stderr","text":["2025/08/01 18:39:08 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n","2025/08/01 18:39:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"]},{"output_type":"stream","name":"stdout","text":["\n","PHASE Phase2_Time RESULTS:\n","Test MAE: 0.09\n","Test RMSE: 0.17\n","Test MAPE: 48.40%\n","Best Val Loss: 0.0594\n","Model Parameters: 425\n","\n","============================================================\n","PHASE: Phase3_Holidays\n","Description: DLinear with time and holiday features\n","Features: 8 features\n","============================================================\n","Creating dataset with 8 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas']\n","Created 3957 valid samples\n","Feature scaling check:\n","  Input shape: torch.Size([20, 9])\n","  Target (col 0) range: -0.055 to 1.814\n","  Features (col 1+) range: 0.000 to 24.000\n","Checking feature variation across samples:\n","  Sample 0 features: [ 3.9  14.5   1.6   0.05  0.05  0.    0.    0.  ]\n","  Sample 1 features: [ 4.1  15.5   1.65  0.05  0.05  0.    0.    0.  ]\n","  Sample 2 features: [ 4.35 16.5   1.75  0.    0.    0.    0.    0.  ]\n","  Sample 3 features: [ 4.6  17.5   1.85  0.    0.    0.    0.    0.  ]\n","  Sample 4 features: [ 4.85 18.5   1.95  0.    0.    0.    0.    0.  ]\n","Creating dataset with 8 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas']\n","Created 726 valid samples\n","Creating dataset with 8 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas']\n","Created 770 valid samples\n","Train samples: 3957\n","Val samples: 726\n","Test samples: 770\n","Expected features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas']\n","Expected n_features: 9\n","Actual input features from dataset: 9\n","Creating model with 9 input features...\n","Creating DLinear with 9 input channels\n","Added feature projection layer: 9 -> 1\n","Model created with 430 parameters\n","Sample input shape: torch.Size([1, 20, 9])\n","Sample target shape: torch.Size([10])\n","Model output shape: torch.Size([1, 10, 1])\n","Expected output shape: [1, 10, 1]\n","Training DLinear model...\n","Training with features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas']\n","Model expects 9 features\n","üöÄ Training DLinear on cuda\n","Feature weights stats: mean=0.0127, std=0.0200\n","Feature weights range: 0.0001 to 0.0850\n","Feature weights stats: mean=0.0926, std=0.0831\n","Feature weights range: 0.0087 to 0.3618\n","Feature weights stats: mean=0.3052, std=0.0919\n","Feature weights range: 0.1413 to 0.5159\n","Epoch 10/50, Train Loss: 0.0824, Val Loss: 0.0996\n","Feature weights stats: mean=0.5963, std=0.0251\n","Feature weights range: 0.5596 to 0.6581\n","Feature weights stats: mean=0.5900, std=0.0242\n","Feature weights range: 0.5516 to 0.6498\n","Epoch 20/50, Train Loss: 0.0807, Val Loss: 0.0741\n","Feature weights stats: mean=0.5930, std=0.0193\n","Feature weights range: 0.5604 to 0.6399\n","Feature weights stats: mean=0.5940, std=0.0205\n","Feature weights range: 0.5523 to 0.6405\n","Feature weights stats: mean=0.5960, std=0.0174\n","Feature weights range: 0.5682 to 0.6366\n","Feature weights stats: mean=0.5966, std=0.0163\n","Feature weights range: 0.5640 to 0.6403\n","Feature weights stats: mean=0.5891, std=0.0178\n","Feature weights range: 0.5502 to 0.6392\n","Feature weights stats: mean=0.5938, std=0.0149\n","Feature weights range: 0.5655 to 0.6389\n","Feature weights stats: mean=0.5915, std=0.0156\n","Feature weights range: 0.5603 to 0.6342\n","Feature weights stats: mean=0.5981, std=0.0134\n","Feature weights range: 0.5689 to 0.6288\n","Epoch 30/50, Train Loss: 0.0797, Val Loss: 0.0645\n","Feature weights stats: mean=0.5786, std=0.0155\n","Feature weights range: 0.5418 to 0.6137\n","Feature weights stats: mean=0.5830, std=0.0139\n","Feature weights range: 0.5566 to 0.6218\n","Feature weights stats: mean=0.5709, std=0.0185\n","Feature weights range: 0.5336 to 0.6124\n","Epoch 40/50, Train Loss: 0.0803, Val Loss: 0.0644\n","Feature weights stats: mean=0.5889, std=0.0118\n","Feature weights range: 0.5597 to 0.6160\n","Feature weights stats: mean=0.5857, std=0.0131\n","Feature weights range: 0.5518 to 0.6152\n","Feature weights stats: mean=0.5846, std=0.0139\n","Feature weights range: 0.5502 to 0.6197\n","Early stopping at epoch 49\n","DLinear parameters breakdown:\n","  Seasonal Linear: 210\n","  Trend Linear: 210\n","  Feature Projection: 10\n","  Total: 430\n","Evaluating model...\n","Prediction statistics:\n","  Mean: -0.3607\n","  Std: 0.3090\n","  Min: -0.7292\n","  Max: 1.2105\n","  Unique values: 7676\n","Actual statistics:\n","  Mean: -0.3734\n","  Std: 0.3441\n","  Min: -0.7052\n","  Max: 1.7071\n"]},{"output_type":"stream","name":"stderr","text":["2025/08/01 18:40:55 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n","2025/08/01 18:40:58 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"]},{"output_type":"stream","name":"stdout","text":["\n","PHASE Phase3_Holidays RESULTS:\n","Test MAE: 0.09\n","Test RMSE: 0.17\n","Test MAPE: 48.20%\n","Best Val Loss: 0.0606\n","Model Parameters: 430\n","\n","============================================================\n","PHASE: Phase4_External\n","Description: DLinear with time, holiday, and external features\n","Features: 8 features\n","============================================================\n","Creating dataset with 8 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","Created 3957 valid samples\n","Feature scaling check:\n","  Input shape: torch.Size([20, 9])\n","  Target (col 0) range: -0.055 to 1.814\n","  Features (col 1+) range: 0.000 to 211.456\n","Checking feature variation across samples:\n","  Sample 0 features: [3.9000001e+00 1.4500000e+01 1.6000000e+00 5.0000001e-02 6.2691998e+01\n"," 2.7047501e+00 2.1094751e+02 7.9272003e+00]\n","  Sample 1 features: [4.0999999e+00 1.5500000e+01 1.6500000e+00 5.0000001e-02 6.4793503e+01\n"," 2.7087998e+00 2.1095963e+02 7.9123001e+00]\n","  Sample 2 features: [  4.35     16.5       1.75      0.       66.9135    2.71485 210.9587\n","   7.89635]\n","  Sample 3 features: [  4.6        17.5         1.85        0.         68.940994    2.7212498\n"," 210.94966     7.8804   ]\n","  Sample 4 features: [  4.85     18.5       1.95      0.       70.767     2.72435 210.93869\n","   7.86445]\n","Creating dataset with 8 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","Created 726 valid samples\n","Creating dataset with 8 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","Created 770 valid samples\n","Train samples: 3957\n","Val samples: 726\n","Test samples: 770\n","Expected features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","Expected n_features: 9\n","Actual input features from dataset: 9\n","Creating model with 9 input features...\n","Creating DLinear with 9 input channels\n","Added feature projection layer: 9 -> 1\n","Model created with 430 parameters\n","Sample input shape: torch.Size([1, 20, 9])\n","Sample target shape: torch.Size([10])\n","Model output shape: torch.Size([1, 10, 1])\n","Expected output shape: [1, 10, 1]\n","Training DLinear model...\n","Training with features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n","Model expects 9 features\n","üöÄ Training DLinear on cuda\n","Feature weights stats: mean=0.9999, std=0.0001\n","Feature weights range: 0.9997 to 1.0000\n","Epoch 10/50, Train Loss: 0.0841, Val Loss: 0.0599\n","Feature weights stats: mean=1.0000, std=0.0001\n","Feature weights range: 0.9998 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0001\n","Feature weights range: 0.9998 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0001\n","Feature weights range: 0.9998 to 1.0000\n","Epoch 20/50, Train Loss: 0.0811, Val Loss: 0.0578\n","Early stopping at epoch 29\n","DLinear parameters breakdown:\n","  Seasonal Linear: 210\n","  Trend Linear: 210\n","  Feature Projection: 10\n","  Total: 430\n","Evaluating model...\n","Prediction statistics:\n","  Mean: -0.3564\n","  Std: 0.3038\n","  Min: -0.6926\n","  Max: 1.2236\n","  Unique values: 7676\n","Actual statistics:\n","  Mean: -0.3734\n","  Std: 0.3441\n","  Min: -0.7052\n","  Max: 1.7071\n","\n","PHASE Phase4_External RESULTS:\n","Test MAE: 0.09\n","Test RMSE: 0.17\n","Test MAPE: 48.51%\n","Best Val Loss: 0.0573\n","Model Parameters: 430\n","\n","============================================================\n","PHASE: Phase5_Lags\n","Description: DLinear with time and lag features (no Sales_diff1 as per your note)\n","Features: 7 features\n","============================================================\n","Creating dataset with 7 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4']\n","Created 3957 valid samples\n","Feature scaling check:\n","  Input shape: torch.Size([20, 8])\n","  Target (col 0) range: -0.055 to 1.814\n","  Features (col 1+) range: 0.000 to 57258.430\n","Checking feature variation across samples:\n","  Sample 0 features: [3.9000001e+00 1.4500000e+01 1.6000000e+00 5.0000001e-02 2.4976566e+04\n"," 2.5390910e+04 2.6226434e+04]\n","  Sample 1 features: [4.0999999e+00 1.5500000e+01 1.6500000e+00 5.0000001e-02 2.4541152e+04\n"," 2.4976566e+04 2.5759230e+04]\n","  Sample 2 features: [4.3499999e+00 1.6500000e+01 1.7500000e+00 0.0000000e+00 2.4111365e+04\n"," 2.4541152e+04 2.5390910e+04]\n","  Sample 3 features: [4.5999999e+00 1.7500000e+01 1.8500000e+00 0.0000000e+00 2.2626051e+04\n"," 2.4111365e+04 2.4976566e+04]\n","  Sample 4 features: [4.8499999e+00 1.8500000e+01 1.9500000e+00 0.0000000e+00 2.1430709e+04\n"," 2.2626051e+04 2.4541152e+04]\n","Creating dataset with 7 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4']\n","Created 726 valid samples\n","Creating dataset with 7 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4']\n","Created 770 valid samples\n","Train samples: 3957\n","Val samples: 726\n","Test samples: 770\n","Expected features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4']\n","Expected n_features: 8\n","Actual input features from dataset: 8\n","Creating model with 8 input features...\n","Creating DLinear with 8 input channels\n","Added feature projection layer: 8 -> 1\n","Model created with 429 parameters\n","Sample input shape: torch.Size([1, 20, 8])\n","Sample target shape: torch.Size([10])\n","Model output shape: torch.Size([1, 10, 1])\n","Expected output shape: [1, 10, 1]\n","Training DLinear model...\n","Training with features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4']\n","Model expects 8 features\n","üöÄ Training DLinear on cuda\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Epoch 10/50, Train Loss: 0.0834, Val Loss: 0.0571\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Early stopping at epoch 18\n","DLinear parameters breakdown:\n","  Seasonal Linear: 210\n","  Trend Linear: 210\n","  Feature Projection: 9\n","  Total: 429\n","Evaluating model...\n","Prediction statistics:\n","  Mean: -0.3496\n","  Std: 0.2977\n","  Min: -0.6931\n","  Max: 1.1901\n","  Unique values: 7679\n","Actual statistics:\n","  Mean: -0.3734\n","  Std: 0.3441\n","  Min: -0.7052\n","  Max: 1.7071\n","\n","PHASE Phase5_Lags RESULTS:\n","Test MAE: 0.10\n","Test RMSE: 0.17\n","Test MAPE: 49.41%\n","Best Val Loss: 0.0571\n","Model Parameters: 429\n","\n","============================================================\n","PHASE: Phase6_Full\n","Description: DLinear with comprehensive feature set (excluding Sales_diff1)\n","Features: 17 features\n","============================================================\n","Creating dataset with 17 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4', 'store_outlier_propensity', 'dept_outlier_propensity']\n","Created 3957 valid samples\n","Feature scaling check:\n","  Input shape: torch.Size([20, 18])\n","  Target (col 0) range: -0.055 to 1.814\n","  Features (col 1+) range: 0.000 to 57258.430\n","Checking feature variation across samples:\n","  Sample 0 features: [3.9000001e+00 1.4500000e+01 1.6000000e+00 5.0000001e-02 5.0000001e-02\n"," 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.2691998e+01 2.7047501e+00\n"," 2.1094751e+02 7.9272003e+00 2.4976566e+04 2.5390910e+04 2.6226434e+04\n"," 0.0000000e+00 0.0000000e+00]\n","  Sample 1 features: [4.0999999e+00 1.5500000e+01 1.6500000e+00 5.0000001e-02 5.0000001e-02\n"," 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.4793503e+01 2.7087998e+00\n"," 2.1095963e+02 7.9123001e+00 2.4541152e+04 2.4976566e+04 2.5759230e+04\n"," 0.0000000e+00 0.0000000e+00]\n","  Sample 2 features: [4.3499999e+00 1.6500000e+01 1.7500000e+00 0.0000000e+00 0.0000000e+00\n"," 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.6913498e+01 2.7148499e+00\n"," 2.1095869e+02 7.8963499e+00 2.4111365e+04 2.4541152e+04 2.5390910e+04\n"," 0.0000000e+00 0.0000000e+00]\n","  Sample 3 features: [4.5999999e+00 1.7500000e+01 1.8500000e+00 0.0000000e+00 0.0000000e+00\n"," 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.8940994e+01 2.7212498e+00\n"," 2.1094966e+02 7.8804002e+00 2.2626051e+04 2.4111365e+04 2.4976566e+04\n"," 0.0000000e+00 0.0000000e+00]\n","  Sample 4 features: [4.8499999e+00 1.8500000e+01 1.9500000e+00 0.0000000e+00 0.0000000e+00\n"," 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.0766998e+01 2.7243500e+00\n"," 2.1093869e+02 7.8644500e+00 2.1430709e+04 2.2626051e+04 2.4541152e+04\n"," 0.0000000e+00 0.0000000e+00]\n","Creating dataset with 17 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4', 'store_outlier_propensity', 'dept_outlier_propensity']\n","Created 726 valid samples\n","Creating dataset with 17 features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4', 'store_outlier_propensity', 'dept_outlier_propensity']\n","Created 770 valid samples\n","Train samples: 3957\n","Val samples: 726\n","Test samples: 770\n","Expected features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4', 'store_outlier_propensity', 'dept_outlier_propensity']\n","Expected n_features: 18\n","Actual input features from dataset: 18\n","Creating model with 18 input features...\n","Creating DLinear with 18 input channels\n","Added feature projection layer: 18 -> 1\n","Model created with 439 parameters\n","Sample input shape: torch.Size([1, 20, 18])\n","Sample target shape: torch.Size([10])\n","Model output shape: torch.Size([1, 10, 1])\n","Expected output shape: [1, 10, 1]\n","Training DLinear model...\n","Training with features: ['Month', 'Week', 'Quarter', 'IsHoliday', 'Is_SuperBowl', 'Is_LaborDay', 'Is_Thanksgiving', 'Is_Christmas', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_4', 'store_outlier_propensity', 'dept_outlier_propensity']\n","Model expects 18 features\n","üöÄ Training DLinear on cuda\n","Feature weights stats: mean=0.0000, std=0.0000\n","Feature weights range: 0.0000 to 0.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Epoch 10/50, Train Loss: 0.0865, Val Loss: 0.0566\n","Feature weights stats: mean=1.0000, std=0.0000\n","Feature weights range: 1.0000 to 1.0000\n","Early stopping at epoch 18\n","DLinear parameters breakdown:\n","  Seasonal Linear: 210\n","  Trend Linear: 210\n","  Feature Projection: 19\n","  Total: 439\n","Evaluating model...\n","Prediction statistics:\n","  Mean: -0.3341\n","  Std: 0.2873\n","  Min: -0.6890\n","  Max: 0.8125\n","  Unique values: 7677\n","Actual statistics:\n","  Mean: -0.3734\n","  Std: 0.3441\n","  Min: -0.7052\n","  Max: 1.7071\n","\n","PHASE Phase6_Full RESULTS:\n","Test MAE: 0.11\n","Test RMSE: 0.19\n","Test MAPE: 51.15%\n","Best Val Loss: 0.0566\n","Model Parameters: 439\n","\n","üèÜ BEST MODEL: Phase3_Holidays with MAE: 0.09\n"]},{"output_type":"stream","name":"stderr","text":["2025/08/01 18:46:00 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n","2025/08/01 18:46:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n","Successfully registered model 'WalmartSales_DLinear_Best'.\n","2025/08/01 18:46:11 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: WalmartSales_DLinear_Best, version 1\n","Created version '1' of model 'WalmartSales_DLinear_Best'.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Best model registered in MLflow Model Registry\n","Model URI: runs:/2f633bd2cfaa4473b9b27348279475e7/best_dlinear_model\n","\n","================================================================================\n","DLINEAR EXPERIMENT RESULTS SUMMARY\n","================================================================================\n","          Phase                                                          Description  N_Features      MAE     RMSE      MAPE  Val_Loss  Train_Samples  Test_Samples  Parameters\n","Phase3_Holidays                               DLinear with time and holiday features           8 0.086683 0.167945 48.200864  0.060637           3957           770         430\n","    Phase2_Time                                           DLinear with time features           3 0.087307 0.168089 48.398855  0.059416           3957           770         425\n","   Phase1_Basic                     Basic DLinear with only Weekly_Sales time series           0 0.089926 0.169009 48.450547  0.058167           3957           770         420\n","Phase4_External                    DLinear with time, holiday, and external features           8 0.090098 0.169214 48.508841  0.057267           3957           770         430\n","    Phase5_Lags DLinear with time and lag features (no Sales_diff1 as per your note)           7 0.096682 0.171976 49.411714  0.057120           3957           770         429\n","    Phase6_Full       DLinear with comprehensive feature set (excluding Sales_diff1)          17 0.113601 0.190319 51.145142  0.056643           3957           770         439\n","\n","‚úÖ All DLinear experiments completed!\n","üèÜ Best model: Phase3_Holidays with MAE: 0.09\n"]},{"output_type":"error","ename":"SyntaxError","evalue":"'return' outside function (ipython-input-1225536856.py, line 381)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1225536856.py\"\u001b[0;36m, line \u001b[0;32m381\u001b[0m\n\u001b[0;31m    return best_model, best_phase, all_results\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DL09oYN-a_QH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GWjEI1VCa_Sb"},"execution_count":null,"outputs":[]}]}